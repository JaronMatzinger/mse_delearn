{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary # with torchsummary we got a bug, consider installing torchinfo\n",
    "from torch.utils.data import DataLoader, TensorDataset # lets us load data in batches\n",
    "from tqdm.notebook import tqdm # with tqdm we got a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# wget the file from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "with open(\"shakespeare.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus_length = len(corpus)\n",
    "    \n",
    "print(\"Loaded a corpus of {0} characters\".format(corpus_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 65 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Get a unique identifier for each char in the corpus, \n",
    "# then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print(encoding)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One  approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])\n",
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print(decoding[y_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check y. Dimension: torch.Size([1115374]) # Sentences: 1115374 Characters in corpus: 65\n",
      "Sanity check X. Dimension: torch.Size([1115374, 20, 65]) Sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data) #No need to encode labels in one-hot, crossEntropy loss needs just the indexes (not 0-1 values)\n",
    "\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\"\n",
    "      .format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\"\n",
    "      .format(X.size(), sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model\n",
    "        \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.rnn(x)\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1]\n",
    "        # Convert the final state to our desired output shape (batch_size, output_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNNModel                                 --                        --\n",
       "├─RNN: 1-1                               [128, 20, 256]            82,688\n",
       "├─Linear: 1-2                            [128, 65]                 16,705\n",
       "==========================================================================================\n",
       "Total params: 99,393\n",
       "Trainable params: 99,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 213.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 5.31\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 6.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model, input_size=(batch_size, seq_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(65, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode, which disables training specific behaviour.\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from X,y tensors\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214ae6c41554422c82250da90489cf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1115374 (0%)]\tLoss: 4.175115\n",
      "Train Epoch: 1 [25600/1115374 (2%)]\tLoss: 2.911258\n",
      "Train Epoch: 1 [51200/1115374 (5%)]\tLoss: 2.728660\n",
      "Train Epoch: 1 [76800/1115374 (7%)]\tLoss: 2.612786\n",
      "Train Epoch: 1 [102400/1115374 (9%)]\tLoss: 2.061970\n",
      "Train Epoch: 1 [128000/1115374 (11%)]\tLoss: 2.294299\n",
      "Train Epoch: 1 [153600/1115374 (14%)]\tLoss: 2.190934\n",
      "Train Epoch: 1 [179200/1115374 (16%)]\tLoss: 2.257875\n",
      "Train Epoch: 1 [204800/1115374 (18%)]\tLoss: 1.961739\n",
      "Train Epoch: 1 [230400/1115374 (21%)]\tLoss: 2.138092\n",
      "Train Epoch: 1 [256000/1115374 (23%)]\tLoss: 2.287170\n",
      "Train Epoch: 1 [281600/1115374 (25%)]\tLoss: 2.301413\n",
      "Train Epoch: 1 [307200/1115374 (28%)]\tLoss: 2.106874\n",
      "Train Epoch: 1 [332800/1115374 (30%)]\tLoss: 1.976412\n",
      "Train Epoch: 1 [358400/1115374 (32%)]\tLoss: 2.042856\n",
      "Train Epoch: 1 [384000/1115374 (34%)]\tLoss: 1.995202\n",
      "Train Epoch: 1 [409600/1115374 (37%)]\tLoss: 1.906014\n",
      "Train Epoch: 1 [435200/1115374 (39%)]\tLoss: 2.159437\n",
      "Train Epoch: 1 [460800/1115374 (41%)]\tLoss: 2.070092\n",
      "Train Epoch: 1 [486400/1115374 (44%)]\tLoss: 2.360982\n",
      "Train Epoch: 1 [512000/1115374 (46%)]\tLoss: 2.075605\n",
      "Train Epoch: 1 [537600/1115374 (48%)]\tLoss: 1.829672\n",
      "Train Epoch: 1 [563200/1115374 (50%)]\tLoss: 1.991796\n",
      "Train Epoch: 1 [588800/1115374 (53%)]\tLoss: 1.952073\n",
      "Train Epoch: 1 [614400/1115374 (55%)]\tLoss: 1.990022\n",
      "Train Epoch: 1 [640000/1115374 (57%)]\tLoss: 1.907549\n",
      "Train Epoch: 1 [665600/1115374 (60%)]\tLoss: 2.075002\n",
      "Train Epoch: 1 [691200/1115374 (62%)]\tLoss: 2.047622\n",
      "Train Epoch: 1 [716800/1115374 (64%)]\tLoss: 1.978042\n",
      "Train Epoch: 1 [742400/1115374 (67%)]\tLoss: 1.856308\n",
      "Train Epoch: 1 [768000/1115374 (69%)]\tLoss: 2.170775\n",
      "Train Epoch: 1 [793600/1115374 (71%)]\tLoss: 1.992839\n",
      "Train Epoch: 1 [819200/1115374 (73%)]\tLoss: 1.929467\n",
      "Train Epoch: 1 [844800/1115374 (76%)]\tLoss: 1.783949\n",
      "Train Epoch: 1 [870400/1115374 (78%)]\tLoss: 1.815258\n",
      "Train Epoch: 1 [896000/1115374 (80%)]\tLoss: 1.966351\n",
      "Train Epoch: 1 [921600/1115374 (83%)]\tLoss: 1.695050\n",
      "Train Epoch: 1 [947200/1115374 (85%)]\tLoss: 2.053879\n",
      "Train Epoch: 1 [972800/1115374 (87%)]\tLoss: 1.639911\n",
      "Train Epoch: 1 [998400/1115374 (90%)]\tLoss: 1.623078\n",
      "Train Epoch: 1 [1024000/1115374 (92%)]\tLoss: 1.655520\n",
      "Train Epoch: 1 [1049600/1115374 (94%)]\tLoss: 1.904404\n",
      "Train Epoch: 1 [1075200/1115374 (96%)]\tLoss: 1.976895\n",
      "Train Epoch: 1 [1100800/1115374 (99%)]\tLoss: 1.793091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c63482693b4476b1880cde0d95cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/1115374 (0%)]\tLoss: 1.928746\n",
      "Train Epoch: 2 [25600/1115374 (2%)]\tLoss: 1.814796\n",
      "Train Epoch: 2 [51200/1115374 (5%)]\tLoss: 1.712388\n",
      "Train Epoch: 2 [76800/1115374 (7%)]\tLoss: 1.754578\n",
      "Train Epoch: 2 [102400/1115374 (9%)]\tLoss: 1.829680\n",
      "Train Epoch: 2 [128000/1115374 (11%)]\tLoss: 1.648633\n",
      "Train Epoch: 2 [153600/1115374 (14%)]\tLoss: 1.836487\n",
      "Train Epoch: 2 [179200/1115374 (16%)]\tLoss: 1.807741\n",
      "Train Epoch: 2 [204800/1115374 (18%)]\tLoss: 1.765005\n",
      "Train Epoch: 2 [230400/1115374 (21%)]\tLoss: 1.881797\n",
      "Train Epoch: 2 [256000/1115374 (23%)]\tLoss: 1.965522\n",
      "Train Epoch: 2 [281600/1115374 (25%)]\tLoss: 1.813427\n",
      "Train Epoch: 2 [307200/1115374 (28%)]\tLoss: 1.912069\n",
      "Train Epoch: 2 [332800/1115374 (30%)]\tLoss: 1.843701\n",
      "Train Epoch: 2 [358400/1115374 (32%)]\tLoss: 2.003612\n",
      "Train Epoch: 2 [384000/1115374 (34%)]\tLoss: 1.781248\n",
      "Train Epoch: 2 [409600/1115374 (37%)]\tLoss: 1.883123\n",
      "Train Epoch: 2 [435200/1115374 (39%)]\tLoss: 1.880821\n",
      "Train Epoch: 2 [460800/1115374 (41%)]\tLoss: 1.703453\n",
      "Train Epoch: 2 [486400/1115374 (44%)]\tLoss: 1.946741\n",
      "Train Epoch: 2 [512000/1115374 (46%)]\tLoss: 1.812500\n",
      "Train Epoch: 2 [537600/1115374 (48%)]\tLoss: 1.777910\n",
      "Train Epoch: 2 [563200/1115374 (50%)]\tLoss: 1.534819\n",
      "Train Epoch: 2 [588800/1115374 (53%)]\tLoss: 1.520838\n",
      "Train Epoch: 2 [614400/1115374 (55%)]\tLoss: 1.694300\n",
      "Train Epoch: 2 [640000/1115374 (57%)]\tLoss: 1.657609\n",
      "Train Epoch: 2 [665600/1115374 (60%)]\tLoss: 2.014215\n",
      "Train Epoch: 2 [691200/1115374 (62%)]\tLoss: 1.695325\n",
      "Train Epoch: 2 [716800/1115374 (64%)]\tLoss: 1.879775\n",
      "Train Epoch: 2 [742400/1115374 (67%)]\tLoss: 1.569319\n",
      "Train Epoch: 2 [768000/1115374 (69%)]\tLoss: 1.550256\n",
      "Train Epoch: 2 [793600/1115374 (71%)]\tLoss: 1.749097\n",
      "Train Epoch: 2 [819200/1115374 (73%)]\tLoss: 1.459909\n",
      "Train Epoch: 2 [844800/1115374 (76%)]\tLoss: 1.721801\n",
      "Train Epoch: 2 [870400/1115374 (78%)]\tLoss: 1.752449\n",
      "Train Epoch: 2 [896000/1115374 (80%)]\tLoss: 1.545010\n",
      "Train Epoch: 2 [921600/1115374 (83%)]\tLoss: 1.822769\n",
      "Train Epoch: 2 [947200/1115374 (85%)]\tLoss: 1.687443\n",
      "Train Epoch: 2 [972800/1115374 (87%)]\tLoss: 1.572298\n",
      "Train Epoch: 2 [998400/1115374 (90%)]\tLoss: 1.655840\n",
      "Train Epoch: 2 [1024000/1115374 (92%)]\tLoss: 1.774334\n",
      "Train Epoch: 2 [1049600/1115374 (94%)]\tLoss: 1.523798\n",
      "Train Epoch: 2 [1075200/1115374 (96%)]\tLoss: 1.775087\n",
      "Train Epoch: 2 [1100800/1115374 (99%)]\tLoss: 1.805716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db955e2797d4caa8a27cf9326fca0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/1115374 (0%)]\tLoss: 1.668903\n",
      "Train Epoch: 3 [25600/1115374 (2%)]\tLoss: 1.672143\n",
      "Train Epoch: 3 [51200/1115374 (5%)]\tLoss: 1.613708\n",
      "Train Epoch: 3 [76800/1115374 (7%)]\tLoss: 1.705329\n",
      "Train Epoch: 3 [102400/1115374 (9%)]\tLoss: 1.856481\n",
      "Train Epoch: 3 [128000/1115374 (11%)]\tLoss: 1.699763\n",
      "Train Epoch: 3 [153600/1115374 (14%)]\tLoss: 1.779823\n",
      "Train Epoch: 3 [179200/1115374 (16%)]\tLoss: 1.551905\n",
      "Train Epoch: 3 [204800/1115374 (18%)]\tLoss: 1.648129\n",
      "Train Epoch: 3 [230400/1115374 (21%)]\tLoss: 1.842480\n",
      "Train Epoch: 3 [256000/1115374 (23%)]\tLoss: 1.728832\n",
      "Train Epoch: 3 [281600/1115374 (25%)]\tLoss: 1.572739\n",
      "Train Epoch: 3 [307200/1115374 (28%)]\tLoss: 1.735193\n",
      "Train Epoch: 3 [332800/1115374 (30%)]\tLoss: 1.791128\n",
      "Train Epoch: 3 [358400/1115374 (32%)]\tLoss: 1.550185\n",
      "Train Epoch: 3 [384000/1115374 (34%)]\tLoss: 1.665074\n",
      "Train Epoch: 3 [409600/1115374 (37%)]\tLoss: 1.739619\n",
      "Train Epoch: 3 [435200/1115374 (39%)]\tLoss: 1.814051\n",
      "Train Epoch: 3 [460800/1115374 (41%)]\tLoss: 1.807907\n",
      "Train Epoch: 3 [486400/1115374 (44%)]\tLoss: 1.700959\n",
      "Train Epoch: 3 [512000/1115374 (46%)]\tLoss: 1.696367\n",
      "Train Epoch: 3 [537600/1115374 (48%)]\tLoss: 1.699560\n",
      "Train Epoch: 3 [563200/1115374 (50%)]\tLoss: 1.657609\n",
      "Train Epoch: 3 [588800/1115374 (53%)]\tLoss: 1.584414\n",
      "Train Epoch: 3 [614400/1115374 (55%)]\tLoss: 1.586044\n",
      "Train Epoch: 3 [640000/1115374 (57%)]\tLoss: 1.807491\n",
      "Train Epoch: 3 [665600/1115374 (60%)]\tLoss: 1.708957\n",
      "Train Epoch: 3 [691200/1115374 (62%)]\tLoss: 1.729296\n",
      "Train Epoch: 3 [716800/1115374 (64%)]\tLoss: 1.729998\n",
      "Train Epoch: 3 [742400/1115374 (67%)]\tLoss: 1.688214\n",
      "Train Epoch: 3 [768000/1115374 (69%)]\tLoss: 1.724930\n",
      "Train Epoch: 3 [793600/1115374 (71%)]\tLoss: 1.801116\n",
      "Train Epoch: 3 [819200/1115374 (73%)]\tLoss: 1.634250\n",
      "Train Epoch: 3 [844800/1115374 (76%)]\tLoss: 1.688648\n",
      "Train Epoch: 3 [870400/1115374 (78%)]\tLoss: 1.443854\n",
      "Train Epoch: 3 [896000/1115374 (80%)]\tLoss: 1.750550\n",
      "Train Epoch: 3 [921600/1115374 (83%)]\tLoss: 1.495237\n",
      "Train Epoch: 3 [947200/1115374 (85%)]\tLoss: 1.808084\n",
      "Train Epoch: 3 [972800/1115374 (87%)]\tLoss: 1.599317\n",
      "Train Epoch: 3 [998400/1115374 (90%)]\tLoss: 1.992956\n",
      "Train Epoch: 3 [1024000/1115374 (92%)]\tLoss: 1.524252\n",
      "Train Epoch: 3 [1049600/1115374 (94%)]\tLoss: 1.733262\n",
      "Train Epoch: 3 [1075200/1115374 (96%)]\tLoss: 1.601380\n",
      "Train Epoch: 3 [1100800/1115374 (99%)]\tLoss: 1.532002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200a0c508eb54f60abaadfabd4789cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/1115374 (0%)]\tLoss: 1.712696\n",
      "Train Epoch: 4 [25600/1115374 (2%)]\tLoss: 1.402240\n",
      "Train Epoch: 4 [51200/1115374 (5%)]\tLoss: 1.523956\n",
      "Train Epoch: 4 [76800/1115374 (7%)]\tLoss: 1.705879\n",
      "Train Epoch: 4 [102400/1115374 (9%)]\tLoss: 1.452181\n",
      "Train Epoch: 4 [128000/1115374 (11%)]\tLoss: 1.677110\n",
      "Train Epoch: 4 [153600/1115374 (14%)]\tLoss: 1.658741\n",
      "Train Epoch: 4 [179200/1115374 (16%)]\tLoss: 1.601360\n",
      "Train Epoch: 4 [204800/1115374 (18%)]\tLoss: 1.661314\n",
      "Train Epoch: 4 [230400/1115374 (21%)]\tLoss: 1.468249\n",
      "Train Epoch: 4 [256000/1115374 (23%)]\tLoss: 1.507754\n",
      "Train Epoch: 4 [281600/1115374 (25%)]\tLoss: 1.549421\n",
      "Train Epoch: 4 [307200/1115374 (28%)]\tLoss: 1.471640\n",
      "Train Epoch: 4 [332800/1115374 (30%)]\tLoss: 1.491198\n",
      "Train Epoch: 4 [358400/1115374 (32%)]\tLoss: 1.506903\n",
      "Train Epoch: 4 [384000/1115374 (34%)]\tLoss: 1.405229\n",
      "Train Epoch: 4 [409600/1115374 (37%)]\tLoss: 1.616395\n",
      "Train Epoch: 4 [435200/1115374 (39%)]\tLoss: 1.804013\n",
      "Train Epoch: 4 [460800/1115374 (41%)]\tLoss: 1.540321\n",
      "Train Epoch: 4 [486400/1115374 (44%)]\tLoss: 1.587160\n",
      "Train Epoch: 4 [512000/1115374 (46%)]\tLoss: 1.560463\n",
      "Train Epoch: 4 [537600/1115374 (48%)]\tLoss: 1.690982\n",
      "Train Epoch: 4 [563200/1115374 (50%)]\tLoss: 1.575595\n",
      "Train Epoch: 4 [588800/1115374 (53%)]\tLoss: 1.546229\n",
      "Train Epoch: 4 [614400/1115374 (55%)]\tLoss: 1.704997\n",
      "Train Epoch: 4 [640000/1115374 (57%)]\tLoss: 1.527944\n",
      "Train Epoch: 4 [665600/1115374 (60%)]\tLoss: 1.439187\n",
      "Train Epoch: 4 [691200/1115374 (62%)]\tLoss: 1.799056\n",
      "Train Epoch: 4 [716800/1115374 (64%)]\tLoss: 1.560392\n",
      "Train Epoch: 4 [742400/1115374 (67%)]\tLoss: 1.720724\n",
      "Train Epoch: 4 [768000/1115374 (69%)]\tLoss: 1.599462\n",
      "Train Epoch: 4 [793600/1115374 (71%)]\tLoss: 1.651120\n",
      "Train Epoch: 4 [819200/1115374 (73%)]\tLoss: 1.610379\n",
      "Train Epoch: 4 [844800/1115374 (76%)]\tLoss: 1.538468\n",
      "Train Epoch: 4 [870400/1115374 (78%)]\tLoss: 1.550600\n",
      "Train Epoch: 4 [896000/1115374 (80%)]\tLoss: 1.951675\n",
      "Train Epoch: 4 [921600/1115374 (83%)]\tLoss: 1.866127\n",
      "Train Epoch: 4 [947200/1115374 (85%)]\tLoss: 1.749769\n",
      "Train Epoch: 4 [972800/1115374 (87%)]\tLoss: 1.565598\n",
      "Train Epoch: 4 [998400/1115374 (90%)]\tLoss: 1.641820\n",
      "Train Epoch: 4 [1024000/1115374 (92%)]\tLoss: 1.543438\n",
      "Train Epoch: 4 [1049600/1115374 (94%)]\tLoss: 1.621249\n",
      "Train Epoch: 4 [1075200/1115374 (96%)]\tLoss: 1.625477\n",
      "Train Epoch: 4 [1100800/1115374 (99%)]\tLoss: 1.611498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c5d55128eb4c17893f3ae0b210af3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/1115374 (0%)]\tLoss: 1.594130\n",
      "Train Epoch: 5 [25600/1115374 (2%)]\tLoss: 1.450200\n",
      "Train Epoch: 5 [51200/1115374 (5%)]\tLoss: 1.675308\n",
      "Train Epoch: 5 [76800/1115374 (7%)]\tLoss: 1.706847\n",
      "Train Epoch: 5 [102400/1115374 (9%)]\tLoss: 1.572026\n",
      "Train Epoch: 5 [128000/1115374 (11%)]\tLoss: 1.480092\n",
      "Train Epoch: 5 [153600/1115374 (14%)]\tLoss: 1.683562\n",
      "Train Epoch: 5 [179200/1115374 (16%)]\tLoss: 1.524388\n",
      "Train Epoch: 5 [204800/1115374 (18%)]\tLoss: 1.611913\n",
      "Train Epoch: 5 [230400/1115374 (21%)]\tLoss: 1.550364\n",
      "Train Epoch: 5 [256000/1115374 (23%)]\tLoss: 1.689188\n",
      "Train Epoch: 5 [281600/1115374 (25%)]\tLoss: 1.535446\n",
      "Train Epoch: 5 [307200/1115374 (28%)]\tLoss: 1.809318\n",
      "Train Epoch: 5 [332800/1115374 (30%)]\tLoss: 1.395368\n",
      "Train Epoch: 5 [358400/1115374 (32%)]\tLoss: 1.661031\n",
      "Train Epoch: 5 [384000/1115374 (34%)]\tLoss: 1.400563\n",
      "Train Epoch: 5 [409600/1115374 (37%)]\tLoss: 1.488994\n",
      "Train Epoch: 5 [435200/1115374 (39%)]\tLoss: 1.460088\n",
      "Train Epoch: 5 [460800/1115374 (41%)]\tLoss: 1.985615\n",
      "Train Epoch: 5 [486400/1115374 (44%)]\tLoss: 1.686587\n",
      "Train Epoch: 5 [512000/1115374 (46%)]\tLoss: 1.555808\n",
      "Train Epoch: 5 [537600/1115374 (48%)]\tLoss: 1.579013\n",
      "Train Epoch: 5 [563200/1115374 (50%)]\tLoss: 1.232538\n",
      "Train Epoch: 5 [588800/1115374 (53%)]\tLoss: 1.922394\n",
      "Train Epoch: 5 [614400/1115374 (55%)]\tLoss: 1.558562\n",
      "Train Epoch: 5 [640000/1115374 (57%)]\tLoss: 1.553528\n",
      "Train Epoch: 5 [665600/1115374 (60%)]\tLoss: 1.434703\n",
      "Train Epoch: 5 [691200/1115374 (62%)]\tLoss: 1.623541\n",
      "Train Epoch: 5 [716800/1115374 (64%)]\tLoss: 1.582696\n",
      "Train Epoch: 5 [742400/1115374 (67%)]\tLoss: 1.726164\n",
      "Train Epoch: 5 [768000/1115374 (69%)]\tLoss: 1.402397\n",
      "Train Epoch: 5 [793600/1115374 (71%)]\tLoss: 1.652823\n",
      "Train Epoch: 5 [819200/1115374 (73%)]\tLoss: 1.476577\n",
      "Train Epoch: 5 [844800/1115374 (76%)]\tLoss: 1.516858\n",
      "Train Epoch: 5 [870400/1115374 (78%)]\tLoss: 1.577378\n",
      "Train Epoch: 5 [896000/1115374 (80%)]\tLoss: 1.610361\n",
      "Train Epoch: 5 [921600/1115374 (83%)]\tLoss: 1.611002\n",
      "Train Epoch: 5 [947200/1115374 (85%)]\tLoss: 1.291994\n",
      "Train Epoch: 5 [972800/1115374 (87%)]\tLoss: 1.677337\n",
      "Train Epoch: 5 [998400/1115374 (90%)]\tLoss: 1.726681\n",
      "Train Epoch: 5 [1024000/1115374 (92%)]\tLoss: 1.817537\n",
      "Train Epoch: 5 [1049600/1115374 (94%)]\tLoss: 1.666245\n",
      "Train Epoch: 5 [1075200/1115374 (96%)]\tLoss: 1.485057\n",
      "Train Epoch: 5 [1100800/1115374 (99%)]\tLoss: 1.514484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce39adf1b52b4d0fb5fd541e3b1e5ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/1115374 (0%)]\tLoss: 1.713285\n",
      "Train Epoch: 6 [25600/1115374 (2%)]\tLoss: 1.463874\n",
      "Train Epoch: 6 [51200/1115374 (5%)]\tLoss: 1.625275\n",
      "Train Epoch: 6 [76800/1115374 (7%)]\tLoss: 1.431798\n",
      "Train Epoch: 6 [102400/1115374 (9%)]\tLoss: 1.519416\n",
      "Train Epoch: 6 [128000/1115374 (11%)]\tLoss: 1.511877\n",
      "Train Epoch: 6 [153600/1115374 (14%)]\tLoss: 1.597981\n",
      "Train Epoch: 6 [179200/1115374 (16%)]\tLoss: 1.407740\n",
      "Train Epoch: 6 [204800/1115374 (18%)]\tLoss: 1.473077\n",
      "Train Epoch: 6 [230400/1115374 (21%)]\tLoss: 1.513715\n",
      "Train Epoch: 6 [256000/1115374 (23%)]\tLoss: 1.388884\n",
      "Train Epoch: 6 [281600/1115374 (25%)]\tLoss: 1.381631\n",
      "Train Epoch: 6 [307200/1115374 (28%)]\tLoss: 1.810377\n",
      "Train Epoch: 6 [332800/1115374 (30%)]\tLoss: 1.545333\n",
      "Train Epoch: 6 [358400/1115374 (32%)]\tLoss: 1.789157\n",
      "Train Epoch: 6 [384000/1115374 (34%)]\tLoss: 1.711739\n",
      "Train Epoch: 6 [409600/1115374 (37%)]\tLoss: 1.503041\n",
      "Train Epoch: 6 [435200/1115374 (39%)]\tLoss: 1.664795\n",
      "Train Epoch: 6 [460800/1115374 (41%)]\tLoss: 1.578752\n",
      "Train Epoch: 6 [486400/1115374 (44%)]\tLoss: 1.831796\n",
      "Train Epoch: 6 [512000/1115374 (46%)]\tLoss: 1.518960\n",
      "Train Epoch: 6 [537600/1115374 (48%)]\tLoss: 1.743203\n",
      "Train Epoch: 6 [563200/1115374 (50%)]\tLoss: 1.637959\n",
      "Train Epoch: 6 [588800/1115374 (53%)]\tLoss: 1.350746\n",
      "Train Epoch: 6 [614400/1115374 (55%)]\tLoss: 1.293175\n",
      "Train Epoch: 6 [640000/1115374 (57%)]\tLoss: 1.529118\n",
      "Train Epoch: 6 [665600/1115374 (60%)]\tLoss: 1.568837\n",
      "Train Epoch: 6 [691200/1115374 (62%)]\tLoss: 1.415873\n",
      "Train Epoch: 6 [716800/1115374 (64%)]\tLoss: 1.657077\n",
      "Train Epoch: 6 [742400/1115374 (67%)]\tLoss: 1.523314\n",
      "Train Epoch: 6 [768000/1115374 (69%)]\tLoss: 1.447914\n",
      "Train Epoch: 6 [793600/1115374 (71%)]\tLoss: 1.534740\n",
      "Train Epoch: 6 [819200/1115374 (73%)]\tLoss: 1.523617\n",
      "Train Epoch: 6 [844800/1115374 (76%)]\tLoss: 1.359905\n",
      "Train Epoch: 6 [870400/1115374 (78%)]\tLoss: 1.660904\n",
      "Train Epoch: 6 [896000/1115374 (80%)]\tLoss: 1.437334\n",
      "Train Epoch: 6 [921600/1115374 (83%)]\tLoss: 1.578100\n",
      "Train Epoch: 6 [947200/1115374 (85%)]\tLoss: 1.545790\n",
      "Train Epoch: 6 [972800/1115374 (87%)]\tLoss: 1.455564\n",
      "Train Epoch: 6 [998400/1115374 (90%)]\tLoss: 1.551836\n",
      "Train Epoch: 6 [1024000/1115374 (92%)]\tLoss: 1.745628\n",
      "Train Epoch: 6 [1049600/1115374 (94%)]\tLoss: 1.777170\n",
      "Train Epoch: 6 [1075200/1115374 (96%)]\tLoss: 1.438928\n",
      "Train Epoch: 6 [1100800/1115374 (99%)]\tLoss: 1.434237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844165a8087a4947acdeb80cb1292294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/1115374 (0%)]\tLoss: 1.663615\n",
      "Train Epoch: 7 [25600/1115374 (2%)]\tLoss: 1.479217\n",
      "Train Epoch: 7 [51200/1115374 (5%)]\tLoss: 1.310662\n",
      "Train Epoch: 7 [76800/1115374 (7%)]\tLoss: 1.722632\n",
      "Train Epoch: 7 [102400/1115374 (9%)]\tLoss: 1.579577\n",
      "Train Epoch: 7 [128000/1115374 (11%)]\tLoss: 1.885215\n",
      "Train Epoch: 7 [153600/1115374 (14%)]\tLoss: 1.546348\n",
      "Train Epoch: 7 [179200/1115374 (16%)]\tLoss: 1.563493\n",
      "Train Epoch: 7 [204800/1115374 (18%)]\tLoss: 1.594930\n",
      "Train Epoch: 7 [230400/1115374 (21%)]\tLoss: 1.595198\n",
      "Train Epoch: 7 [256000/1115374 (23%)]\tLoss: 1.699292\n",
      "Train Epoch: 7 [281600/1115374 (25%)]\tLoss: 1.441522\n",
      "Train Epoch: 7 [307200/1115374 (28%)]\tLoss: 1.634887\n",
      "Train Epoch: 7 [332800/1115374 (30%)]\tLoss: 1.669103\n",
      "Train Epoch: 7 [358400/1115374 (32%)]\tLoss: 1.399090\n",
      "Train Epoch: 7 [384000/1115374 (34%)]\tLoss: 1.558705\n",
      "Train Epoch: 7 [409600/1115374 (37%)]\tLoss: 1.546654\n",
      "Train Epoch: 7 [435200/1115374 (39%)]\tLoss: 1.635404\n",
      "Train Epoch: 7 [460800/1115374 (41%)]\tLoss: 1.358034\n",
      "Train Epoch: 7 [486400/1115374 (44%)]\tLoss: 1.417507\n",
      "Train Epoch: 7 [512000/1115374 (46%)]\tLoss: 1.526158\n",
      "Train Epoch: 7 [537600/1115374 (48%)]\tLoss: 1.590246\n",
      "Train Epoch: 7 [563200/1115374 (50%)]\tLoss: 1.414175\n",
      "Train Epoch: 7 [588800/1115374 (53%)]\tLoss: 1.464872\n",
      "Train Epoch: 7 [614400/1115374 (55%)]\tLoss: 1.692462\n",
      "Train Epoch: 7 [640000/1115374 (57%)]\tLoss: 1.500896\n",
      "Train Epoch: 7 [665600/1115374 (60%)]\tLoss: 1.556657\n",
      "Train Epoch: 7 [691200/1115374 (62%)]\tLoss: 1.429083\n",
      "Train Epoch: 7 [716800/1115374 (64%)]\tLoss: 1.799079\n",
      "Train Epoch: 7 [742400/1115374 (67%)]\tLoss: 1.560126\n",
      "Train Epoch: 7 [768000/1115374 (69%)]\tLoss: 1.556068\n",
      "Train Epoch: 7 [793600/1115374 (71%)]\tLoss: 1.294614\n",
      "Train Epoch: 7 [819200/1115374 (73%)]\tLoss: 1.512278\n",
      "Train Epoch: 7 [844800/1115374 (76%)]\tLoss: 1.471798\n",
      "Train Epoch: 7 [870400/1115374 (78%)]\tLoss: 1.630316\n",
      "Train Epoch: 7 [896000/1115374 (80%)]\tLoss: 1.709981\n",
      "Train Epoch: 7 [921600/1115374 (83%)]\tLoss: 1.529855\n",
      "Train Epoch: 7 [947200/1115374 (85%)]\tLoss: 1.693192\n",
      "Train Epoch: 7 [972800/1115374 (87%)]\tLoss: 1.502622\n",
      "Train Epoch: 7 [998400/1115374 (90%)]\tLoss: 1.771455\n",
      "Train Epoch: 7 [1024000/1115374 (92%)]\tLoss: 1.561845\n",
      "Train Epoch: 7 [1049600/1115374 (94%)]\tLoss: 1.361851\n",
      "Train Epoch: 7 [1075200/1115374 (96%)]\tLoss: 1.443741\n",
      "Train Epoch: 7 [1100800/1115374 (99%)]\tLoss: 1.475478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4dd6d172014c3484943d9a6a584254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/1115374 (0%)]\tLoss: 1.495067\n",
      "Train Epoch: 8 [25600/1115374 (2%)]\tLoss: 1.621270\n",
      "Train Epoch: 8 [51200/1115374 (5%)]\tLoss: 1.590618\n",
      "Train Epoch: 8 [76800/1115374 (7%)]\tLoss: 1.416974\n",
      "Train Epoch: 8 [102400/1115374 (9%)]\tLoss: 1.689593\n",
      "Train Epoch: 8 [128000/1115374 (11%)]\tLoss: 1.512234\n",
      "Train Epoch: 8 [153600/1115374 (14%)]\tLoss: 1.598617\n",
      "Train Epoch: 8 [179200/1115374 (16%)]\tLoss: 1.465475\n",
      "Train Epoch: 8 [204800/1115374 (18%)]\tLoss: 1.440889\n",
      "Train Epoch: 8 [230400/1115374 (21%)]\tLoss: 1.661847\n",
      "Train Epoch: 8 [256000/1115374 (23%)]\tLoss: 1.693488\n",
      "Train Epoch: 8 [281600/1115374 (25%)]\tLoss: 1.597530\n",
      "Train Epoch: 8 [307200/1115374 (28%)]\tLoss: 1.619745\n",
      "Train Epoch: 8 [332800/1115374 (30%)]\tLoss: 1.490320\n",
      "Train Epoch: 8 [358400/1115374 (32%)]\tLoss: 1.471078\n",
      "Train Epoch: 8 [384000/1115374 (34%)]\tLoss: 1.640102\n",
      "Train Epoch: 8 [409600/1115374 (37%)]\tLoss: 1.429859\n",
      "Train Epoch: 8 [435200/1115374 (39%)]\tLoss: 1.664646\n",
      "Train Epoch: 8 [460800/1115374 (41%)]\tLoss: 1.410570\n",
      "Train Epoch: 8 [486400/1115374 (44%)]\tLoss: 1.422043\n",
      "Train Epoch: 8 [512000/1115374 (46%)]\tLoss: 1.444280\n",
      "Train Epoch: 8 [537600/1115374 (48%)]\tLoss: 1.626617\n",
      "Train Epoch: 8 [563200/1115374 (50%)]\tLoss: 1.313664\n",
      "Train Epoch: 8 [588800/1115374 (53%)]\tLoss: 1.614646\n",
      "Train Epoch: 8 [614400/1115374 (55%)]\tLoss: 1.356643\n",
      "Train Epoch: 8 [640000/1115374 (57%)]\tLoss: 1.619202\n",
      "Train Epoch: 8 [665600/1115374 (60%)]\tLoss: 1.390625\n",
      "Train Epoch: 8 [691200/1115374 (62%)]\tLoss: 1.510249\n",
      "Train Epoch: 8 [716800/1115374 (64%)]\tLoss: 1.266377\n",
      "Train Epoch: 8 [742400/1115374 (67%)]\tLoss: 1.642355\n",
      "Train Epoch: 8 [768000/1115374 (69%)]\tLoss: 1.571996\n",
      "Train Epoch: 8 [793600/1115374 (71%)]\tLoss: 1.393202\n",
      "Train Epoch: 8 [819200/1115374 (73%)]\tLoss: 1.582048\n",
      "Train Epoch: 8 [844800/1115374 (76%)]\tLoss: 1.562068\n",
      "Train Epoch: 8 [870400/1115374 (78%)]\tLoss: 1.600464\n",
      "Train Epoch: 8 [896000/1115374 (80%)]\tLoss: 1.536098\n",
      "Train Epoch: 8 [921600/1115374 (83%)]\tLoss: 1.638665\n",
      "Train Epoch: 8 [947200/1115374 (85%)]\tLoss: 1.323514\n",
      "Train Epoch: 8 [972800/1115374 (87%)]\tLoss: 1.492342\n",
      "Train Epoch: 8 [998400/1115374 (90%)]\tLoss: 1.618962\n",
      "Train Epoch: 8 [1024000/1115374 (92%)]\tLoss: 1.461189\n",
      "Train Epoch: 8 [1049600/1115374 (94%)]\tLoss: 1.531864\n",
      "Train Epoch: 8 [1075200/1115374 (96%)]\tLoss: 1.844301\n",
      "Train Epoch: 8 [1100800/1115374 (99%)]\tLoss: 1.234845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce56deebe75d4aebbbe704655d2d4cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/1115374 (0%)]\tLoss: 1.499055\n",
      "Train Epoch: 9 [25600/1115374 (2%)]\tLoss: 1.309863\n",
      "Train Epoch: 9 [51200/1115374 (5%)]\tLoss: 1.662575\n",
      "Train Epoch: 9 [76800/1115374 (7%)]\tLoss: 1.630345\n",
      "Train Epoch: 9 [102400/1115374 (9%)]\tLoss: 1.412812\n",
      "Train Epoch: 9 [128000/1115374 (11%)]\tLoss: 1.421527\n",
      "Train Epoch: 9 [153600/1115374 (14%)]\tLoss: 1.780044\n",
      "Train Epoch: 9 [179200/1115374 (16%)]\tLoss: 1.617279\n",
      "Train Epoch: 9 [204800/1115374 (18%)]\tLoss: 1.479558\n",
      "Train Epoch: 9 [230400/1115374 (21%)]\tLoss: 1.549310\n",
      "Train Epoch: 9 [256000/1115374 (23%)]\tLoss: 1.382309\n",
      "Train Epoch: 9 [281600/1115374 (25%)]\tLoss: 1.503865\n",
      "Train Epoch: 9 [307200/1115374 (28%)]\tLoss: 1.603572\n",
      "Train Epoch: 9 [332800/1115374 (30%)]\tLoss: 1.402100\n",
      "Train Epoch: 9 [358400/1115374 (32%)]\tLoss: 1.586578\n",
      "Train Epoch: 9 [384000/1115374 (34%)]\tLoss: 1.456884\n",
      "Train Epoch: 9 [409600/1115374 (37%)]\tLoss: 1.413990\n",
      "Train Epoch: 9 [435200/1115374 (39%)]\tLoss: 1.848335\n",
      "Train Epoch: 9 [460800/1115374 (41%)]\tLoss: 1.313638\n",
      "Train Epoch: 9 [486400/1115374 (44%)]\tLoss: 1.553018\n",
      "Train Epoch: 9 [512000/1115374 (46%)]\tLoss: 1.277740\n",
      "Train Epoch: 9 [537600/1115374 (48%)]\tLoss: 1.491281\n",
      "Train Epoch: 9 [563200/1115374 (50%)]\tLoss: 1.341668\n",
      "Train Epoch: 9 [588800/1115374 (53%)]\tLoss: 1.538279\n",
      "Train Epoch: 9 [614400/1115374 (55%)]\tLoss: 1.579605\n",
      "Train Epoch: 9 [640000/1115374 (57%)]\tLoss: 1.530357\n",
      "Train Epoch: 9 [665600/1115374 (60%)]\tLoss: 1.510808\n",
      "Train Epoch: 9 [691200/1115374 (62%)]\tLoss: 1.451867\n",
      "Train Epoch: 9 [716800/1115374 (64%)]\tLoss: 1.666956\n",
      "Train Epoch: 9 [742400/1115374 (67%)]\tLoss: 1.520442\n",
      "Train Epoch: 9 [768000/1115374 (69%)]\tLoss: 1.540460\n",
      "Train Epoch: 9 [793600/1115374 (71%)]\tLoss: 1.549376\n",
      "Train Epoch: 9 [819200/1115374 (73%)]\tLoss: 1.395605\n",
      "Train Epoch: 9 [844800/1115374 (76%)]\tLoss: 1.753387\n",
      "Train Epoch: 9 [870400/1115374 (78%)]\tLoss: 1.673376\n",
      "Train Epoch: 9 [896000/1115374 (80%)]\tLoss: 1.725901\n",
      "Train Epoch: 9 [921600/1115374 (83%)]\tLoss: 1.523161\n",
      "Train Epoch: 9 [947200/1115374 (85%)]\tLoss: 1.367719\n",
      "Train Epoch: 9 [972800/1115374 (87%)]\tLoss: 1.493057\n",
      "Train Epoch: 9 [998400/1115374 (90%)]\tLoss: 1.645294\n",
      "Train Epoch: 9 [1024000/1115374 (92%)]\tLoss: 1.384534\n",
      "Train Epoch: 9 [1049600/1115374 (94%)]\tLoss: 1.738890\n",
      "Train Epoch: 9 [1075200/1115374 (96%)]\tLoss: 1.387138\n",
      "Train Epoch: 9 [1100800/1115374 (99%)]\tLoss: 1.499560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddb853a74ca4588b0644dc275cef9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/1115374 (0%)]\tLoss: 1.517670\n",
      "Train Epoch: 10 [25600/1115374 (2%)]\tLoss: 1.521443\n",
      "Train Epoch: 10 [51200/1115374 (5%)]\tLoss: 1.457784\n",
      "Train Epoch: 10 [76800/1115374 (7%)]\tLoss: 1.487198\n",
      "Train Epoch: 10 [102400/1115374 (9%)]\tLoss: 1.341290\n",
      "Train Epoch: 10 [128000/1115374 (11%)]\tLoss: 1.505288\n",
      "Train Epoch: 10 [153600/1115374 (14%)]\tLoss: 1.486324\n",
      "Train Epoch: 10 [179200/1115374 (16%)]\tLoss: 1.595999\n",
      "Train Epoch: 10 [204800/1115374 (18%)]\tLoss: 1.544000\n",
      "Train Epoch: 10 [230400/1115374 (21%)]\tLoss: 1.482180\n",
      "Train Epoch: 10 [256000/1115374 (23%)]\tLoss: 1.388148\n",
      "Train Epoch: 10 [281600/1115374 (25%)]\tLoss: 1.732257\n",
      "Train Epoch: 10 [307200/1115374 (28%)]\tLoss: 1.525519\n",
      "Train Epoch: 10 [332800/1115374 (30%)]\tLoss: 1.640902\n",
      "Train Epoch: 10 [358400/1115374 (32%)]\tLoss: 1.260059\n",
      "Train Epoch: 10 [384000/1115374 (34%)]\tLoss: 1.520008\n",
      "Train Epoch: 10 [409600/1115374 (37%)]\tLoss: 1.593517\n",
      "Train Epoch: 10 [435200/1115374 (39%)]\tLoss: 1.637178\n",
      "Train Epoch: 10 [460800/1115374 (41%)]\tLoss: 1.579327\n",
      "Train Epoch: 10 [486400/1115374 (44%)]\tLoss: 1.172126\n",
      "Train Epoch: 10 [512000/1115374 (46%)]\tLoss: 1.883549\n",
      "Train Epoch: 10 [537600/1115374 (48%)]\tLoss: 1.499663\n",
      "Train Epoch: 10 [563200/1115374 (50%)]\tLoss: 1.692153\n",
      "Train Epoch: 10 [588800/1115374 (53%)]\tLoss: 1.595533\n",
      "Train Epoch: 10 [614400/1115374 (55%)]\tLoss: 1.624741\n",
      "Train Epoch: 10 [640000/1115374 (57%)]\tLoss: 1.425221\n",
      "Train Epoch: 10 [665600/1115374 (60%)]\tLoss: 1.584254\n",
      "Train Epoch: 10 [691200/1115374 (62%)]\tLoss: 1.374048\n",
      "Train Epoch: 10 [716800/1115374 (64%)]\tLoss: 1.396253\n",
      "Train Epoch: 10 [742400/1115374 (67%)]\tLoss: 1.492933\n",
      "Train Epoch: 10 [768000/1115374 (69%)]\tLoss: 1.381190\n",
      "Train Epoch: 10 [793600/1115374 (71%)]\tLoss: 1.429586\n",
      "Train Epoch: 10 [819200/1115374 (73%)]\tLoss: 1.565665\n",
      "Train Epoch: 10 [844800/1115374 (76%)]\tLoss: 1.567113\n",
      "Train Epoch: 10 [870400/1115374 (78%)]\tLoss: 1.310662\n",
      "Train Epoch: 10 [896000/1115374 (80%)]\tLoss: 1.851930\n",
      "Train Epoch: 10 [921600/1115374 (83%)]\tLoss: 1.450404\n",
      "Train Epoch: 10 [947200/1115374 (85%)]\tLoss: 1.253487\n",
      "Train Epoch: 10 [972800/1115374 (87%)]\tLoss: 1.416105\n",
      "Train Epoch: 10 [998400/1115374 (90%)]\tLoss: 1.469893\n",
      "Train Epoch: 10 [1024000/1115374 (92%)]\tLoss: 1.578375\n",
      "Train Epoch: 10 [1049600/1115374 (94%)]\tLoss: 1.512874\n",
      "Train Epoch: 10 [1075200/1115374 (96%)]\tLoss: 1.595130\n",
      "Train Epoch: 10 [1100800/1115374 (99%)]\tLoss: 1.500032\n",
      "CPU times: user 3min 28s, sys: 3.65 s, total: 3min 31s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "%%time\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, model, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = Once upon a time in \n",
      "me.\n",
      "\n",
      "KING RICHARD II:\n",
      "I am learnedeh\n",
      "To dear mes quept the like a strikt.\n",
      "\n",
      "KATHARINA:\n",
      "Yours but advitius; braved.\n",
      "\n",
      "HERMIONE:\n",
      "Ay, for they do you do? Comy mainer it for this pletry to your fair that tord pite it:\n",
      "Their contresed was counted will past, I am concest; I do throath, for what's spiant?\n",
      "\n",
      "HENRY:\n",
      "Rilf:\n",
      "Well I ad his tongue objest name of our engmain'd the poor liventionious Mays\n",
      "Them first was cannot; and three I may care\n",
      "Farewell conspirits be come, I had rocking at you:\n",
      "Mire, trare's m\n"
     ]
    }
   ],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "    \n",
    "    \n",
    "\n",
    "seed_pattern = make_seed(\"Once upon a time in \")\n",
    "print(\"seed = \" + seed_pattern)\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    # predict() gives a tensor of shape (1, 65) \n",
    "    # with 1 being the size of the batch, for that we use [0] to get a vector\n",
    "    output = predict(model, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    # Randomly choose from a multinomial distribution with the output probabilities\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    \n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i+1:i+1 + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append([encoding[char] for char in next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "['i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print([decoding[idx] for idx in y_data[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model\n",
    "class mmRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    # TO DO: check the many-to-many configuration\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.rnn(x)\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1]\n",
    "        # Convert the final state to our desired output shape (batch_size, output_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelmm = mmRNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(modelmm, input_size=(batch_size, seq_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training time\n",
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode, which disables training specific behaviour.\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from X,y tensors\n",
    "datasett = TensorDataset(X, y)\n",
    "train_loader = DataLoader(datasett, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "%%time\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, modelmm, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"In the early morning, the flower is shining\")\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output = predict(modelmm, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    # Randomly choose from a multinomial distribution with the output probabilities\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    \n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
