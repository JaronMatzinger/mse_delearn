{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary # with torchsummary we got a bug, consider installing torchinfo\n",
    "from torch.utils.data import DataLoader, TensorDataset # lets us load data in batches\n",
    "from tqdm.notebook import tqdm # with tqdm we got a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# wget the file from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "with open(\"shakespeare.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus_length = len(corpus)\n",
    "    \n",
    "print(\"Loaded a corpus of {0} characters\".format(corpus_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 65 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Get a unique identifier for each char in the corpus, \n",
    "# then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "print(decoding)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One  approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])\n",
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print(decoding[y_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check y. Dimension: torch.Size([1115374]) # Sentences: 1115374 Characters in corpus: 65\n",
      "Sanity check X. Dimension: torch.Size([1115374, 20, 65]) Sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data) #No need to encode labels in one-hot with pytorch,\n",
    "                         #crossEntropy loss needs just the indexes (not 0-1 values)\n",
    "\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\"\n",
    "      .format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\"\n",
    "      .format(X.size(), sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model\n",
    "        \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.rnn(x)\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1]\n",
    "        # Convert the final state to our desired output shape (batch_size, output_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNNModel                                 --                        --\n",
       "├─RNN: 1-1                               [128, 20, 256]            82,688\n",
       "├─Linear: 1-2                            [128, 65]                 16,705\n",
       "==========================================================================================\n",
       "Total params: 99,393\n",
       "Trainable params: 99,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 213.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 5.31\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 6.37\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model, input_size=(batch_size, seq_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(65, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backpropagate. Updates the gradients buffer on each parameter\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode, which disables training specific behaviour.\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from X,y tensors\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a5f29dbc64bdd8416ab6fdecbbbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1115374 (0%)]\tLoss: 4.169981\n",
      "Train Epoch: 1 [25600/1115374 (2%)]\tLoss: 2.681995\n",
      "Train Epoch: 1 [51200/1115374 (5%)]\tLoss: 2.469980\n",
      "Train Epoch: 1 [76800/1115374 (7%)]\tLoss: 2.224135\n",
      "Train Epoch: 1 [102400/1115374 (9%)]\tLoss: 2.281017\n",
      "Train Epoch: 1 [128000/1115374 (11%)]\tLoss: 2.175488\n",
      "Train Epoch: 1 [153600/1115374 (14%)]\tLoss: 2.363190\n",
      "Train Epoch: 1 [179200/1115374 (16%)]\tLoss: 2.362417\n",
      "Train Epoch: 1 [204800/1115374 (18%)]\tLoss: 2.395211\n",
      "Train Epoch: 1 [230400/1115374 (21%)]\tLoss: 2.076811\n",
      "Train Epoch: 1 [256000/1115374 (23%)]\tLoss: 2.212866\n",
      "Train Epoch: 1 [281600/1115374 (25%)]\tLoss: 1.969840\n",
      "Train Epoch: 1 [307200/1115374 (28%)]\tLoss: 2.132294\n",
      "Train Epoch: 1 [332800/1115374 (30%)]\tLoss: 2.136962\n",
      "Train Epoch: 1 [358400/1115374 (32%)]\tLoss: 2.033324\n",
      "Train Epoch: 1 [384000/1115374 (34%)]\tLoss: 2.021786\n",
      "Train Epoch: 1 [409600/1115374 (37%)]\tLoss: 1.968149\n",
      "Train Epoch: 1 [435200/1115374 (39%)]\tLoss: 2.089594\n",
      "Train Epoch: 1 [460800/1115374 (41%)]\tLoss: 1.896635\n",
      "Train Epoch: 1 [486400/1115374 (44%)]\tLoss: 2.270048\n",
      "Train Epoch: 1 [512000/1115374 (46%)]\tLoss: 2.113833\n",
      "Train Epoch: 1 [537600/1115374 (48%)]\tLoss: 1.950098\n",
      "Train Epoch: 1 [563200/1115374 (50%)]\tLoss: 2.103801\n",
      "Train Epoch: 1 [588800/1115374 (53%)]\tLoss: 1.879630\n",
      "Train Epoch: 1 [614400/1115374 (55%)]\tLoss: 2.051762\n",
      "Train Epoch: 1 [640000/1115374 (57%)]\tLoss: 1.879392\n",
      "Train Epoch: 1 [665600/1115374 (60%)]\tLoss: 1.950693\n",
      "Train Epoch: 1 [691200/1115374 (62%)]\tLoss: 2.075116\n",
      "Train Epoch: 1 [716800/1115374 (64%)]\tLoss: 1.975854\n",
      "Train Epoch: 1 [742400/1115374 (67%)]\tLoss: 1.813565\n",
      "Train Epoch: 1 [768000/1115374 (69%)]\tLoss: 1.808166\n",
      "Train Epoch: 1 [793600/1115374 (71%)]\tLoss: 2.263428\n",
      "Train Epoch: 1 [819200/1115374 (73%)]\tLoss: 1.896463\n",
      "Train Epoch: 1 [844800/1115374 (76%)]\tLoss: 1.783356\n",
      "Train Epoch: 1 [870400/1115374 (78%)]\tLoss: 1.918786\n",
      "Train Epoch: 1 [896000/1115374 (80%)]\tLoss: 2.238873\n",
      "Train Epoch: 1 [921600/1115374 (83%)]\tLoss: 2.025663\n",
      "Train Epoch: 1 [947200/1115374 (85%)]\tLoss: 1.986878\n",
      "Train Epoch: 1 [972800/1115374 (87%)]\tLoss: 1.883536\n",
      "Train Epoch: 1 [998400/1115374 (90%)]\tLoss: 1.544263\n",
      "Train Epoch: 1 [1024000/1115374 (92%)]\tLoss: 1.642202\n",
      "Train Epoch: 1 [1049600/1115374 (94%)]\tLoss: 1.809883\n",
      "Train Epoch: 1 [1075200/1115374 (96%)]\tLoss: 1.780860\n",
      "Train Epoch: 1 [1100800/1115374 (99%)]\tLoss: 1.828011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97ffa9ada2b408abe21aac1281ae864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/1115374 (0%)]\tLoss: 1.893206\n",
      "Train Epoch: 2 [25600/1115374 (2%)]\tLoss: 1.747950\n",
      "Train Epoch: 2 [51200/1115374 (5%)]\tLoss: 1.497042\n",
      "Train Epoch: 2 [76800/1115374 (7%)]\tLoss: 1.979671\n",
      "Train Epoch: 2 [102400/1115374 (9%)]\tLoss: 1.838284\n",
      "Train Epoch: 2 [128000/1115374 (11%)]\tLoss: 1.733971\n",
      "Train Epoch: 2 [153600/1115374 (14%)]\tLoss: 1.942205\n",
      "Train Epoch: 2 [179200/1115374 (16%)]\tLoss: 1.814615\n",
      "Train Epoch: 2 [204800/1115374 (18%)]\tLoss: 1.849527\n",
      "Train Epoch: 2 [230400/1115374 (21%)]\tLoss: 1.844924\n",
      "Train Epoch: 2 [256000/1115374 (23%)]\tLoss: 1.859700\n",
      "Train Epoch: 2 [281600/1115374 (25%)]\tLoss: 1.633758\n",
      "Train Epoch: 2 [307200/1115374 (28%)]\tLoss: 1.789626\n",
      "Train Epoch: 2 [332800/1115374 (30%)]\tLoss: 1.494280\n",
      "Train Epoch: 2 [358400/1115374 (32%)]\tLoss: 1.745963\n",
      "Train Epoch: 2 [384000/1115374 (34%)]\tLoss: 1.589289\n",
      "Train Epoch: 2 [409600/1115374 (37%)]\tLoss: 1.610713\n",
      "Train Epoch: 2 [435200/1115374 (39%)]\tLoss: 1.664986\n",
      "Train Epoch: 2 [460800/1115374 (41%)]\tLoss: 1.988114\n",
      "Train Epoch: 2 [486400/1115374 (44%)]\tLoss: 1.924905\n",
      "Train Epoch: 2 [512000/1115374 (46%)]\tLoss: 2.008046\n",
      "Train Epoch: 2 [537600/1115374 (48%)]\tLoss: 1.686547\n",
      "Train Epoch: 2 [563200/1115374 (50%)]\tLoss: 1.677968\n",
      "Train Epoch: 2 [588800/1115374 (53%)]\tLoss: 1.883522\n",
      "Train Epoch: 2 [614400/1115374 (55%)]\tLoss: 1.705066\n",
      "Train Epoch: 2 [640000/1115374 (57%)]\tLoss: 1.871312\n",
      "Train Epoch: 2 [665600/1115374 (60%)]\tLoss: 1.639745\n",
      "Train Epoch: 2 [691200/1115374 (62%)]\tLoss: 1.615256\n",
      "Train Epoch: 2 [716800/1115374 (64%)]\tLoss: 1.526666\n",
      "Train Epoch: 2 [742400/1115374 (67%)]\tLoss: 1.795714\n",
      "Train Epoch: 2 [768000/1115374 (69%)]\tLoss: 1.683101\n",
      "Train Epoch: 2 [793600/1115374 (71%)]\tLoss: 1.691303\n",
      "Train Epoch: 2 [819200/1115374 (73%)]\tLoss: 1.756868\n",
      "Train Epoch: 2 [844800/1115374 (76%)]\tLoss: 1.705535\n",
      "Train Epoch: 2 [870400/1115374 (78%)]\tLoss: 1.776794\n",
      "Train Epoch: 2 [896000/1115374 (80%)]\tLoss: 1.702176\n",
      "Train Epoch: 2 [921600/1115374 (83%)]\tLoss: 1.628290\n",
      "Train Epoch: 2 [947200/1115374 (85%)]\tLoss: 1.656956\n",
      "Train Epoch: 2 [972800/1115374 (87%)]\tLoss: 1.896004\n",
      "Train Epoch: 2 [998400/1115374 (90%)]\tLoss: 1.776003\n",
      "Train Epoch: 2 [1024000/1115374 (92%)]\tLoss: 1.637763\n",
      "Train Epoch: 2 [1049600/1115374 (94%)]\tLoss: 1.646048\n",
      "Train Epoch: 2 [1075200/1115374 (96%)]\tLoss: 1.588359\n",
      "Train Epoch: 2 [1100800/1115374 (99%)]\tLoss: 1.635446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073c36b5788491fb7227126adc783ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/1115374 (0%)]\tLoss: 1.664692\n",
      "Train Epoch: 3 [25600/1115374 (2%)]\tLoss: 1.620970\n",
      "Train Epoch: 3 [51200/1115374 (5%)]\tLoss: 1.690218\n",
      "Train Epoch: 3 [76800/1115374 (7%)]\tLoss: 1.650244\n",
      "Train Epoch: 3 [102400/1115374 (9%)]\tLoss: 1.873940\n",
      "Train Epoch: 3 [128000/1115374 (11%)]\tLoss: 1.581694\n",
      "Train Epoch: 3 [153600/1115374 (14%)]\tLoss: 1.954471\n",
      "Train Epoch: 3 [179200/1115374 (16%)]\tLoss: 1.527580\n",
      "Train Epoch: 3 [204800/1115374 (18%)]\tLoss: 1.455660\n",
      "Train Epoch: 3 [230400/1115374 (21%)]\tLoss: 1.352105\n",
      "Train Epoch: 3 [256000/1115374 (23%)]\tLoss: 1.701287\n",
      "Train Epoch: 3 [281600/1115374 (25%)]\tLoss: 1.623415\n",
      "Train Epoch: 3 [307200/1115374 (28%)]\tLoss: 1.420686\n",
      "Train Epoch: 3 [332800/1115374 (30%)]\tLoss: 1.723725\n",
      "Train Epoch: 3 [358400/1115374 (32%)]\tLoss: 1.722481\n",
      "Train Epoch: 3 [384000/1115374 (34%)]\tLoss: 1.655469\n",
      "Train Epoch: 3 [409600/1115374 (37%)]\tLoss: 1.813828\n",
      "Train Epoch: 3 [435200/1115374 (39%)]\tLoss: 1.791356\n",
      "Train Epoch: 3 [460800/1115374 (41%)]\tLoss: 1.711065\n",
      "Train Epoch: 3 [486400/1115374 (44%)]\tLoss: 1.558026\n",
      "Train Epoch: 3 [512000/1115374 (46%)]\tLoss: 1.708714\n",
      "Train Epoch: 3 [537600/1115374 (48%)]\tLoss: 1.585575\n",
      "Train Epoch: 3 [563200/1115374 (50%)]\tLoss: 1.465443\n",
      "Train Epoch: 3 [588800/1115374 (53%)]\tLoss: 1.675397\n",
      "Train Epoch: 3 [614400/1115374 (55%)]\tLoss: 1.530720\n",
      "Train Epoch: 3 [640000/1115374 (57%)]\tLoss: 1.849279\n",
      "Train Epoch: 3 [665600/1115374 (60%)]\tLoss: 1.661240\n",
      "Train Epoch: 3 [691200/1115374 (62%)]\tLoss: 1.609781\n",
      "Train Epoch: 3 [716800/1115374 (64%)]\tLoss: 1.641935\n",
      "Train Epoch: 3 [742400/1115374 (67%)]\tLoss: 1.901382\n",
      "Train Epoch: 3 [768000/1115374 (69%)]\tLoss: 1.746677\n",
      "Train Epoch: 3 [793600/1115374 (71%)]\tLoss: 1.579865\n",
      "Train Epoch: 3 [819200/1115374 (73%)]\tLoss: 1.568947\n",
      "Train Epoch: 3 [844800/1115374 (76%)]\tLoss: 1.634872\n",
      "Train Epoch: 3 [870400/1115374 (78%)]\tLoss: 1.506008\n",
      "Train Epoch: 3 [896000/1115374 (80%)]\tLoss: 1.526545\n",
      "Train Epoch: 3 [921600/1115374 (83%)]\tLoss: 1.549640\n",
      "Train Epoch: 3 [947200/1115374 (85%)]\tLoss: 1.514759\n",
      "Train Epoch: 3 [972800/1115374 (87%)]\tLoss: 1.517265\n",
      "Train Epoch: 3 [998400/1115374 (90%)]\tLoss: 1.511843\n",
      "Train Epoch: 3 [1024000/1115374 (92%)]\tLoss: 1.604622\n",
      "Train Epoch: 3 [1049600/1115374 (94%)]\tLoss: 1.516505\n",
      "Train Epoch: 3 [1075200/1115374 (96%)]\tLoss: 1.639074\n",
      "Train Epoch: 3 [1100800/1115374 (99%)]\tLoss: 1.589594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c92408c4b3648d2bc03adea2dee2287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/1115374 (0%)]\tLoss: 1.628476\n",
      "Train Epoch: 4 [25600/1115374 (2%)]\tLoss: 1.673342\n",
      "Train Epoch: 4 [51200/1115374 (5%)]\tLoss: 1.568448\n",
      "Train Epoch: 4 [76800/1115374 (7%)]\tLoss: 1.698754\n",
      "Train Epoch: 4 [102400/1115374 (9%)]\tLoss: 1.625651\n",
      "Train Epoch: 4 [128000/1115374 (11%)]\tLoss: 1.331128\n",
      "Train Epoch: 4 [153600/1115374 (14%)]\tLoss: 1.626625\n",
      "Train Epoch: 4 [179200/1115374 (16%)]\tLoss: 1.477084\n",
      "Train Epoch: 4 [204800/1115374 (18%)]\tLoss: 1.515971\n",
      "Train Epoch: 4 [230400/1115374 (21%)]\tLoss: 1.538875\n",
      "Train Epoch: 4 [256000/1115374 (23%)]\tLoss: 1.710585\n",
      "Train Epoch: 4 [281600/1115374 (25%)]\tLoss: 1.845536\n",
      "Train Epoch: 4 [307200/1115374 (28%)]\tLoss: 1.615008\n",
      "Train Epoch: 4 [332800/1115374 (30%)]\tLoss: 1.522805\n",
      "Train Epoch: 4 [358400/1115374 (32%)]\tLoss: 1.484212\n",
      "Train Epoch: 4 [384000/1115374 (34%)]\tLoss: 1.633295\n",
      "Train Epoch: 4 [409600/1115374 (37%)]\tLoss: 1.608667\n",
      "Train Epoch: 4 [435200/1115374 (39%)]\tLoss: 1.613888\n",
      "Train Epoch: 4 [460800/1115374 (41%)]\tLoss: 1.548276\n",
      "Train Epoch: 4 [486400/1115374 (44%)]\tLoss: 1.587500\n",
      "Train Epoch: 4 [512000/1115374 (46%)]\tLoss: 1.369257\n",
      "Train Epoch: 4 [537600/1115374 (48%)]\tLoss: 1.718868\n",
      "Train Epoch: 4 [563200/1115374 (50%)]\tLoss: 1.561731\n",
      "Train Epoch: 4 [588800/1115374 (53%)]\tLoss: 1.621256\n",
      "Train Epoch: 4 [614400/1115374 (55%)]\tLoss: 1.702628\n",
      "Train Epoch: 4 [640000/1115374 (57%)]\tLoss: 1.390452\n",
      "Train Epoch: 4 [665600/1115374 (60%)]\tLoss: 1.714176\n",
      "Train Epoch: 4 [691200/1115374 (62%)]\tLoss: 1.441190\n",
      "Train Epoch: 4 [716800/1115374 (64%)]\tLoss: 1.585715\n",
      "Train Epoch: 4 [742400/1115374 (67%)]\tLoss: 1.470760\n",
      "Train Epoch: 4 [768000/1115374 (69%)]\tLoss: 1.493003\n",
      "Train Epoch: 4 [793600/1115374 (71%)]\tLoss: 1.578149\n",
      "Train Epoch: 4 [819200/1115374 (73%)]\tLoss: 1.628210\n",
      "Train Epoch: 4 [844800/1115374 (76%)]\tLoss: 1.608939\n",
      "Train Epoch: 4 [870400/1115374 (78%)]\tLoss: 1.767666\n",
      "Train Epoch: 4 [896000/1115374 (80%)]\tLoss: 1.628398\n",
      "Train Epoch: 4 [921600/1115374 (83%)]\tLoss: 1.437986\n",
      "Train Epoch: 4 [947200/1115374 (85%)]\tLoss: 1.714698\n",
      "Train Epoch: 4 [972800/1115374 (87%)]\tLoss: 1.692555\n",
      "Train Epoch: 4 [998400/1115374 (90%)]\tLoss: 1.528221\n",
      "Train Epoch: 4 [1024000/1115374 (92%)]\tLoss: 1.708408\n",
      "Train Epoch: 4 [1049600/1115374 (94%)]\tLoss: 1.617025\n",
      "Train Epoch: 4 [1075200/1115374 (96%)]\tLoss: 1.553757\n",
      "Train Epoch: 4 [1100800/1115374 (99%)]\tLoss: 1.803728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a8430ba51542589289061669dd5378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/1115374 (0%)]\tLoss: 1.663267\n",
      "Train Epoch: 5 [25600/1115374 (2%)]\tLoss: 1.650827\n",
      "Train Epoch: 5 [51200/1115374 (5%)]\tLoss: 1.562723\n",
      "Train Epoch: 5 [76800/1115374 (7%)]\tLoss: 1.387399\n",
      "Train Epoch: 5 [102400/1115374 (9%)]\tLoss: 1.441072\n",
      "Train Epoch: 5 [128000/1115374 (11%)]\tLoss: 1.544817\n",
      "Train Epoch: 5 [153600/1115374 (14%)]\tLoss: 1.618398\n",
      "Train Epoch: 5 [179200/1115374 (16%)]\tLoss: 1.633166\n",
      "Train Epoch: 5 [204800/1115374 (18%)]\tLoss: 1.547117\n",
      "Train Epoch: 5 [230400/1115374 (21%)]\tLoss: 1.344917\n",
      "Train Epoch: 5 [256000/1115374 (23%)]\tLoss: 1.581954\n",
      "Train Epoch: 5 [281600/1115374 (25%)]\tLoss: 1.401630\n",
      "Train Epoch: 5 [307200/1115374 (28%)]\tLoss: 1.600782\n",
      "Train Epoch: 5 [332800/1115374 (30%)]\tLoss: 1.544958\n",
      "Train Epoch: 5 [358400/1115374 (32%)]\tLoss: 1.439140\n",
      "Train Epoch: 5 [384000/1115374 (34%)]\tLoss: 1.718422\n",
      "Train Epoch: 5 [409600/1115374 (37%)]\tLoss: 1.454385\n",
      "Train Epoch: 5 [435200/1115374 (39%)]\tLoss: 1.528686\n",
      "Train Epoch: 5 [460800/1115374 (41%)]\tLoss: 1.473672\n",
      "Train Epoch: 5 [486400/1115374 (44%)]\tLoss: 1.562296\n",
      "Train Epoch: 5 [512000/1115374 (46%)]\tLoss: 1.739146\n",
      "Train Epoch: 5 [537600/1115374 (48%)]\tLoss: 1.624887\n",
      "Train Epoch: 5 [563200/1115374 (50%)]\tLoss: 1.525312\n",
      "Train Epoch: 5 [588800/1115374 (53%)]\tLoss: 1.388771\n",
      "Train Epoch: 5 [614400/1115374 (55%)]\tLoss: 1.391782\n",
      "Train Epoch: 5 [640000/1115374 (57%)]\tLoss: 1.689299\n",
      "Train Epoch: 5 [665600/1115374 (60%)]\tLoss: 1.782854\n",
      "Train Epoch: 5 [691200/1115374 (62%)]\tLoss: 1.515741\n",
      "Train Epoch: 5 [716800/1115374 (64%)]\tLoss: 1.516224\n",
      "Train Epoch: 5 [742400/1115374 (67%)]\tLoss: 1.494754\n",
      "Train Epoch: 5 [768000/1115374 (69%)]\tLoss: 1.610714\n",
      "Train Epoch: 5 [793600/1115374 (71%)]\tLoss: 1.498563\n",
      "Train Epoch: 5 [819200/1115374 (73%)]\tLoss: 1.689292\n",
      "Train Epoch: 5 [844800/1115374 (76%)]\tLoss: 1.622635\n",
      "Train Epoch: 5 [870400/1115374 (78%)]\tLoss: 1.721783\n",
      "Train Epoch: 5 [896000/1115374 (80%)]\tLoss: 1.593620\n",
      "Train Epoch: 5 [921600/1115374 (83%)]\tLoss: 1.349533\n",
      "Train Epoch: 5 [947200/1115374 (85%)]\tLoss: 1.800050\n",
      "Train Epoch: 5 [972800/1115374 (87%)]\tLoss: 1.660165\n",
      "Train Epoch: 5 [998400/1115374 (90%)]\tLoss: 1.619226\n",
      "Train Epoch: 5 [1024000/1115374 (92%)]\tLoss: 1.745382\n",
      "Train Epoch: 5 [1049600/1115374 (94%)]\tLoss: 1.582585\n",
      "Train Epoch: 5 [1075200/1115374 (96%)]\tLoss: 1.591749\n",
      "Train Epoch: 5 [1100800/1115374 (99%)]\tLoss: 1.719504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9287fc0785ae4e8192783ff357eca308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/1115374 (0%)]\tLoss: 1.455825\n",
      "Train Epoch: 6 [25600/1115374 (2%)]\tLoss: 1.395231\n",
      "Train Epoch: 6 [51200/1115374 (5%)]\tLoss: 1.348149\n",
      "Train Epoch: 6 [76800/1115374 (7%)]\tLoss: 1.412148\n",
      "Train Epoch: 6 [102400/1115374 (9%)]\tLoss: 1.817682\n",
      "Train Epoch: 6 [128000/1115374 (11%)]\tLoss: 1.650083\n",
      "Train Epoch: 6 [153600/1115374 (14%)]\tLoss: 1.443524\n",
      "Train Epoch: 6 [179200/1115374 (16%)]\tLoss: 1.637295\n",
      "Train Epoch: 6 [204800/1115374 (18%)]\tLoss: 1.591505\n",
      "Train Epoch: 6 [230400/1115374 (21%)]\tLoss: 1.694021\n",
      "Train Epoch: 6 [256000/1115374 (23%)]\tLoss: 1.263211\n",
      "Train Epoch: 6 [281600/1115374 (25%)]\tLoss: 1.528619\n",
      "Train Epoch: 6 [307200/1115374 (28%)]\tLoss: 1.524690\n",
      "Train Epoch: 6 [332800/1115374 (30%)]\tLoss: 1.651365\n",
      "Train Epoch: 6 [358400/1115374 (32%)]\tLoss: 1.541546\n",
      "Train Epoch: 6 [384000/1115374 (34%)]\tLoss: 1.527930\n",
      "Train Epoch: 6 [409600/1115374 (37%)]\tLoss: 1.744090\n",
      "Train Epoch: 6 [435200/1115374 (39%)]\tLoss: 1.608756\n",
      "Train Epoch: 6 [460800/1115374 (41%)]\tLoss: 1.453132\n",
      "Train Epoch: 6 [486400/1115374 (44%)]\tLoss: 1.696959\n",
      "Train Epoch: 6 [512000/1115374 (46%)]\tLoss: 1.542985\n",
      "Train Epoch: 6 [537600/1115374 (48%)]\tLoss: 1.364310\n",
      "Train Epoch: 6 [563200/1115374 (50%)]\tLoss: 1.468807\n",
      "Train Epoch: 6 [588800/1115374 (53%)]\tLoss: 1.692633\n",
      "Train Epoch: 6 [614400/1115374 (55%)]\tLoss: 1.844172\n",
      "Train Epoch: 6 [640000/1115374 (57%)]\tLoss: 1.479875\n",
      "Train Epoch: 6 [665600/1115374 (60%)]\tLoss: 1.611310\n",
      "Train Epoch: 6 [691200/1115374 (62%)]\tLoss: 1.397752\n",
      "Train Epoch: 6 [716800/1115374 (64%)]\tLoss: 1.500383\n",
      "Train Epoch: 6 [742400/1115374 (67%)]\tLoss: 1.681347\n",
      "Train Epoch: 6 [768000/1115374 (69%)]\tLoss: 1.672084\n",
      "Train Epoch: 6 [793600/1115374 (71%)]\tLoss: 1.525561\n",
      "Train Epoch: 6 [819200/1115374 (73%)]\tLoss: 1.618992\n",
      "Train Epoch: 6 [844800/1115374 (76%)]\tLoss: 1.708473\n",
      "Train Epoch: 6 [870400/1115374 (78%)]\tLoss: 1.680032\n",
      "Train Epoch: 6 [896000/1115374 (80%)]\tLoss: 1.402977\n",
      "Train Epoch: 6 [921600/1115374 (83%)]\tLoss: 1.392659\n",
      "Train Epoch: 6 [947200/1115374 (85%)]\tLoss: 1.576688\n",
      "Train Epoch: 6 [972800/1115374 (87%)]\tLoss: 1.641489\n",
      "Train Epoch: 6 [998400/1115374 (90%)]\tLoss: 1.375610\n",
      "Train Epoch: 6 [1024000/1115374 (92%)]\tLoss: 1.445133\n",
      "Train Epoch: 6 [1049600/1115374 (94%)]\tLoss: 1.722808\n",
      "Train Epoch: 6 [1075200/1115374 (96%)]\tLoss: 1.335664\n",
      "Train Epoch: 6 [1100800/1115374 (99%)]\tLoss: 1.326040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6d7637467842e5b0c3a917d71ed73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/1115374 (0%)]\tLoss: 1.572369\n",
      "Train Epoch: 7 [25600/1115374 (2%)]\tLoss: 1.613103\n",
      "Train Epoch: 7 [51200/1115374 (5%)]\tLoss: 1.539670\n",
      "Train Epoch: 7 [76800/1115374 (7%)]\tLoss: 1.491674\n",
      "Train Epoch: 7 [102400/1115374 (9%)]\tLoss: 1.763354\n",
      "Train Epoch: 7 [128000/1115374 (11%)]\tLoss: 1.569413\n",
      "Train Epoch: 7 [153600/1115374 (14%)]\tLoss: 1.507401\n",
      "Train Epoch: 7 [179200/1115374 (16%)]\tLoss: 1.882342\n",
      "Train Epoch: 7 [204800/1115374 (18%)]\tLoss: 1.611797\n",
      "Train Epoch: 7 [230400/1115374 (21%)]\tLoss: 1.535619\n",
      "Train Epoch: 7 [256000/1115374 (23%)]\tLoss: 1.783667\n",
      "Train Epoch: 7 [281600/1115374 (25%)]\tLoss: 1.491929\n",
      "Train Epoch: 7 [307200/1115374 (28%)]\tLoss: 1.543199\n",
      "Train Epoch: 7 [332800/1115374 (30%)]\tLoss: 1.481814\n",
      "Train Epoch: 7 [358400/1115374 (32%)]\tLoss: 1.387993\n",
      "Train Epoch: 7 [384000/1115374 (34%)]\tLoss: 1.532188\n",
      "Train Epoch: 7 [409600/1115374 (37%)]\tLoss: 1.467701\n",
      "Train Epoch: 7 [435200/1115374 (39%)]\tLoss: 1.737835\n",
      "Train Epoch: 7 [460800/1115374 (41%)]\tLoss: 1.487697\n",
      "Train Epoch: 7 [486400/1115374 (44%)]\tLoss: 1.451942\n",
      "Train Epoch: 7 [512000/1115374 (46%)]\tLoss: 1.687736\n",
      "Train Epoch: 7 [537600/1115374 (48%)]\tLoss: 1.375912\n",
      "Train Epoch: 7 [563200/1115374 (50%)]\tLoss: 1.337402\n",
      "Train Epoch: 7 [588800/1115374 (53%)]\tLoss: 1.494198\n",
      "Train Epoch: 7 [614400/1115374 (55%)]\tLoss: 1.524741\n",
      "Train Epoch: 7 [640000/1115374 (57%)]\tLoss: 1.622756\n",
      "Train Epoch: 7 [665600/1115374 (60%)]\tLoss: 1.528624\n",
      "Train Epoch: 7 [691200/1115374 (62%)]\tLoss: 1.717472\n",
      "Train Epoch: 7 [716800/1115374 (64%)]\tLoss: 1.387896\n",
      "Train Epoch: 7 [742400/1115374 (67%)]\tLoss: 1.585750\n",
      "Train Epoch: 7 [768000/1115374 (69%)]\tLoss: 1.441861\n",
      "Train Epoch: 7 [793600/1115374 (71%)]\tLoss: 1.595972\n",
      "Train Epoch: 7 [819200/1115374 (73%)]\tLoss: 1.677970\n",
      "Train Epoch: 7 [844800/1115374 (76%)]\tLoss: 1.574006\n",
      "Train Epoch: 7 [870400/1115374 (78%)]\tLoss: 1.738501\n",
      "Train Epoch: 7 [896000/1115374 (80%)]\tLoss: 1.572565\n",
      "Train Epoch: 7 [921600/1115374 (83%)]\tLoss: 1.403608\n",
      "Train Epoch: 7 [947200/1115374 (85%)]\tLoss: 1.406410\n",
      "Train Epoch: 7 [972800/1115374 (87%)]\tLoss: 1.449898\n",
      "Train Epoch: 7 [998400/1115374 (90%)]\tLoss: 1.632572\n",
      "Train Epoch: 7 [1024000/1115374 (92%)]\tLoss: 1.655115\n",
      "Train Epoch: 7 [1049600/1115374 (94%)]\tLoss: 1.577950\n",
      "Train Epoch: 7 [1075200/1115374 (96%)]\tLoss: 1.426710\n",
      "Train Epoch: 7 [1100800/1115374 (99%)]\tLoss: 1.586665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0eac9dd10f431d8624039907ce96a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/1115374 (0%)]\tLoss: 1.394031\n",
      "Train Epoch: 8 [25600/1115374 (2%)]\tLoss: 1.722870\n",
      "Train Epoch: 8 [51200/1115374 (5%)]\tLoss: 1.519711\n",
      "Train Epoch: 8 [76800/1115374 (7%)]\tLoss: 1.510985\n",
      "Train Epoch: 8 [102400/1115374 (9%)]\tLoss: 1.671777\n",
      "Train Epoch: 8 [128000/1115374 (11%)]\tLoss: 1.394284\n",
      "Train Epoch: 8 [153600/1115374 (14%)]\tLoss: 1.642728\n",
      "Train Epoch: 8 [179200/1115374 (16%)]\tLoss: 1.483497\n",
      "Train Epoch: 8 [204800/1115374 (18%)]\tLoss: 1.613371\n",
      "Train Epoch: 8 [230400/1115374 (21%)]\tLoss: 1.621354\n",
      "Train Epoch: 8 [256000/1115374 (23%)]\tLoss: 1.532810\n",
      "Train Epoch: 8 [281600/1115374 (25%)]\tLoss: 1.464196\n",
      "Train Epoch: 8 [307200/1115374 (28%)]\tLoss: 1.557190\n",
      "Train Epoch: 8 [332800/1115374 (30%)]\tLoss: 1.382360\n",
      "Train Epoch: 8 [358400/1115374 (32%)]\tLoss: 1.326828\n",
      "Train Epoch: 8 [384000/1115374 (34%)]\tLoss: 1.661571\n",
      "Train Epoch: 8 [409600/1115374 (37%)]\tLoss: 1.444420\n",
      "Train Epoch: 8 [435200/1115374 (39%)]\tLoss: 1.512709\n",
      "Train Epoch: 8 [460800/1115374 (41%)]\tLoss: 1.585636\n",
      "Train Epoch: 8 [486400/1115374 (44%)]\tLoss: 1.668247\n",
      "Train Epoch: 8 [512000/1115374 (46%)]\tLoss: 1.630862\n",
      "Train Epoch: 8 [537600/1115374 (48%)]\tLoss: 1.422538\n",
      "Train Epoch: 8 [563200/1115374 (50%)]\tLoss: 1.568573\n",
      "Train Epoch: 8 [588800/1115374 (53%)]\tLoss: 1.510405\n",
      "Train Epoch: 8 [614400/1115374 (55%)]\tLoss: 1.586169\n",
      "Train Epoch: 8 [640000/1115374 (57%)]\tLoss: 1.616639\n",
      "Train Epoch: 8 [665600/1115374 (60%)]\tLoss: 1.300997\n",
      "Train Epoch: 8 [691200/1115374 (62%)]\tLoss: 1.551016\n",
      "Train Epoch: 8 [716800/1115374 (64%)]\tLoss: 1.545660\n",
      "Train Epoch: 8 [742400/1115374 (67%)]\tLoss: 1.496672\n",
      "Train Epoch: 8 [768000/1115374 (69%)]\tLoss: 1.294680\n",
      "Train Epoch: 8 [793600/1115374 (71%)]\tLoss: 1.573450\n",
      "Train Epoch: 8 [819200/1115374 (73%)]\tLoss: 1.674556\n",
      "Train Epoch: 8 [844800/1115374 (76%)]\tLoss: 1.459049\n",
      "Train Epoch: 8 [870400/1115374 (78%)]\tLoss: 1.588083\n",
      "Train Epoch: 8 [896000/1115374 (80%)]\tLoss: 1.696547\n",
      "Train Epoch: 8 [921600/1115374 (83%)]\tLoss: 1.438422\n",
      "Train Epoch: 8 [947200/1115374 (85%)]\tLoss: 1.483878\n",
      "Train Epoch: 8 [972800/1115374 (87%)]\tLoss: 1.610768\n",
      "Train Epoch: 8 [998400/1115374 (90%)]\tLoss: 1.465859\n",
      "Train Epoch: 8 [1024000/1115374 (92%)]\tLoss: 1.472554\n",
      "Train Epoch: 8 [1049600/1115374 (94%)]\tLoss: 1.432370\n",
      "Train Epoch: 8 [1075200/1115374 (96%)]\tLoss: 1.457308\n",
      "Train Epoch: 8 [1100800/1115374 (99%)]\tLoss: 1.500665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692666f9e1e0497bbeba2c6681c76acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/1115374 (0%)]\tLoss: 1.247814\n",
      "Train Epoch: 9 [25600/1115374 (2%)]\tLoss: 1.563518\n",
      "Train Epoch: 9 [51200/1115374 (5%)]\tLoss: 1.745325\n",
      "Train Epoch: 9 [76800/1115374 (7%)]\tLoss: 1.704841\n",
      "Train Epoch: 9 [102400/1115374 (9%)]\tLoss: 1.519532\n",
      "Train Epoch: 9 [128000/1115374 (11%)]\tLoss: 1.779722\n",
      "Train Epoch: 9 [153600/1115374 (14%)]\tLoss: 1.397050\n",
      "Train Epoch: 9 [179200/1115374 (16%)]\tLoss: 1.787843\n",
      "Train Epoch: 9 [204800/1115374 (18%)]\tLoss: 1.517735\n",
      "Train Epoch: 9 [230400/1115374 (21%)]\tLoss: 1.417898\n",
      "Train Epoch: 9 [256000/1115374 (23%)]\tLoss: 1.627772\n",
      "Train Epoch: 9 [281600/1115374 (25%)]\tLoss: 1.350984\n",
      "Train Epoch: 9 [307200/1115374 (28%)]\tLoss: 1.609475\n",
      "Train Epoch: 9 [332800/1115374 (30%)]\tLoss: 1.429184\n",
      "Train Epoch: 9 [358400/1115374 (32%)]\tLoss: 1.351100\n",
      "Train Epoch: 9 [384000/1115374 (34%)]\tLoss: 1.430286\n",
      "Train Epoch: 9 [409600/1115374 (37%)]\tLoss: 1.433340\n",
      "Train Epoch: 9 [435200/1115374 (39%)]\tLoss: 1.766405\n",
      "Train Epoch: 9 [460800/1115374 (41%)]\tLoss: 1.514033\n",
      "Train Epoch: 9 [486400/1115374 (44%)]\tLoss: 1.469441\n",
      "Train Epoch: 9 [512000/1115374 (46%)]\tLoss: 1.266741\n",
      "Train Epoch: 9 [537600/1115374 (48%)]\tLoss: 1.431336\n",
      "Train Epoch: 9 [563200/1115374 (50%)]\tLoss: 1.480564\n",
      "Train Epoch: 9 [588800/1115374 (53%)]\tLoss: 1.571037\n",
      "Train Epoch: 9 [614400/1115374 (55%)]\tLoss: 1.543647\n",
      "Train Epoch: 9 [640000/1115374 (57%)]\tLoss: 1.516838\n",
      "Train Epoch: 9 [665600/1115374 (60%)]\tLoss: 1.562124\n",
      "Train Epoch: 9 [691200/1115374 (62%)]\tLoss: 1.682863\n",
      "Train Epoch: 9 [716800/1115374 (64%)]\tLoss: 1.478761\n",
      "Train Epoch: 9 [742400/1115374 (67%)]\tLoss: 1.326495\n",
      "Train Epoch: 9 [768000/1115374 (69%)]\tLoss: 1.564029\n",
      "Train Epoch: 9 [793600/1115374 (71%)]\tLoss: 1.352071\n",
      "Train Epoch: 9 [819200/1115374 (73%)]\tLoss: 1.640018\n",
      "Train Epoch: 9 [844800/1115374 (76%)]\tLoss: 1.519426\n",
      "Train Epoch: 9 [870400/1115374 (78%)]\tLoss: 1.477119\n",
      "Train Epoch: 9 [896000/1115374 (80%)]\tLoss: 1.515985\n",
      "Train Epoch: 9 [921600/1115374 (83%)]\tLoss: 1.716554\n",
      "Train Epoch: 9 [947200/1115374 (85%)]\tLoss: 1.613191\n",
      "Train Epoch: 9 [972800/1115374 (87%)]\tLoss: 1.656174\n",
      "Train Epoch: 9 [998400/1115374 (90%)]\tLoss: 1.541209\n",
      "Train Epoch: 9 [1024000/1115374 (92%)]\tLoss: 1.575908\n",
      "Train Epoch: 9 [1049600/1115374 (94%)]\tLoss: 1.482531\n",
      "Train Epoch: 9 [1075200/1115374 (96%)]\tLoss: 1.601278\n",
      "Train Epoch: 9 [1100800/1115374 (99%)]\tLoss: 1.522904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc2f49c53c9465b8575da0a4fa80c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/1115374 (0%)]\tLoss: 1.633830\n",
      "Train Epoch: 10 [25600/1115374 (2%)]\tLoss: 1.504240\n",
      "Train Epoch: 10 [51200/1115374 (5%)]\tLoss: 1.612590\n",
      "Train Epoch: 10 [76800/1115374 (7%)]\tLoss: 1.515229\n",
      "Train Epoch: 10 [102400/1115374 (9%)]\tLoss: 1.343975\n",
      "Train Epoch: 10 [128000/1115374 (11%)]\tLoss: 1.854024\n",
      "Train Epoch: 10 [153600/1115374 (14%)]\tLoss: 1.419483\n",
      "Train Epoch: 10 [179200/1115374 (16%)]\tLoss: 1.478558\n",
      "Train Epoch: 10 [204800/1115374 (18%)]\tLoss: 1.545510\n",
      "Train Epoch: 10 [230400/1115374 (21%)]\tLoss: 1.596822\n",
      "Train Epoch: 10 [256000/1115374 (23%)]\tLoss: 1.465683\n",
      "Train Epoch: 10 [281600/1115374 (25%)]\tLoss: 1.738907\n",
      "Train Epoch: 10 [307200/1115374 (28%)]\tLoss: 1.496520\n",
      "Train Epoch: 10 [332800/1115374 (30%)]\tLoss: 1.562315\n",
      "Train Epoch: 10 [358400/1115374 (32%)]\tLoss: 1.580261\n",
      "Train Epoch: 10 [384000/1115374 (34%)]\tLoss: 1.424862\n",
      "Train Epoch: 10 [409600/1115374 (37%)]\tLoss: 1.244634\n",
      "Train Epoch: 10 [435200/1115374 (39%)]\tLoss: 1.789104\n",
      "Train Epoch: 10 [460800/1115374 (41%)]\tLoss: 1.587310\n",
      "Train Epoch: 10 [486400/1115374 (44%)]\tLoss: 1.568849\n",
      "Train Epoch: 10 [512000/1115374 (46%)]\tLoss: 1.385946\n",
      "Train Epoch: 10 [537600/1115374 (48%)]\tLoss: 1.490144\n",
      "Train Epoch: 10 [563200/1115374 (50%)]\tLoss: 1.647900\n",
      "Train Epoch: 10 [588800/1115374 (53%)]\tLoss: 1.245037\n",
      "Train Epoch: 10 [614400/1115374 (55%)]\tLoss: 1.535732\n",
      "Train Epoch: 10 [640000/1115374 (57%)]\tLoss: 1.419335\n",
      "Train Epoch: 10 [665600/1115374 (60%)]\tLoss: 1.758016\n",
      "Train Epoch: 10 [691200/1115374 (62%)]\tLoss: 1.422003\n",
      "Train Epoch: 10 [716800/1115374 (64%)]\tLoss: 1.399306\n",
      "Train Epoch: 10 [742400/1115374 (67%)]\tLoss: 1.375341\n",
      "Train Epoch: 10 [768000/1115374 (69%)]\tLoss: 1.373253\n",
      "Train Epoch: 10 [793600/1115374 (71%)]\tLoss: 1.306074\n",
      "Train Epoch: 10 [819200/1115374 (73%)]\tLoss: 1.387794\n",
      "Train Epoch: 10 [844800/1115374 (76%)]\tLoss: 1.625903\n",
      "Train Epoch: 10 [870400/1115374 (78%)]\tLoss: 1.307145\n",
      "Train Epoch: 10 [896000/1115374 (80%)]\tLoss: 1.555375\n",
      "Train Epoch: 10 [921600/1115374 (83%)]\tLoss: 1.391227\n",
      "Train Epoch: 10 [947200/1115374 (85%)]\tLoss: 1.444363\n",
      "Train Epoch: 10 [972800/1115374 (87%)]\tLoss: 1.577375\n",
      "Train Epoch: 10 [998400/1115374 (90%)]\tLoss: 1.906362\n",
      "Train Epoch: 10 [1024000/1115374 (92%)]\tLoss: 1.517009\n",
      "Train Epoch: 10 [1049600/1115374 (94%)]\tLoss: 1.606481\n",
      "Train Epoch: 10 [1075200/1115374 (96%)]\tLoss: 1.241100\n",
      "Train Epoch: 10 [1100800/1115374 (99%)]\tLoss: 1.336317\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "#%%time\n",
    "\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, model, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = Once upon a time in \n",
      "me than I am love port's as intopt,\n",
      "Thy soul stay, us and gentle poor decext post that firsublo's sist mine!\n",
      "Anch, Romer on thee, yet he knee\n",
      "full true heir of moce to Richard.\n",
      "\n",
      "POMPEY:\n",
      "Now entent they so, my lord.\n",
      "\n",
      "ROMEO:\n",
      "Pay's hungling awaicother his quaints of man than\n",
      "A find'd so let us appreice,\n",
      "And father we was fell will -wompedly sifel king,\n",
      "thou hath were is is his frath, or present and govisuim in minimable\n",
      "And pleasare livine mear grievens for\n",
      "thee.\n",
      "\n",
      "LEONTES:\n",
      "There's Couss'd, stay you\n"
     ]
    }
   ],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "    \n",
    "    \n",
    "\n",
    "seed_pattern = make_seed(\"Once upon a time in \")\n",
    "print(\"seed = \" + seed_pattern)\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    # predict() gives a tensor of shape (1, 65) \n",
    "    # with 1 being the size of the batch, for that we use [0] to get a vector\n",
    "    output = predict(model, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    # Randomly choose from a multinomial distribution with the output probabilities\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    \n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 1115374 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i+1:i+1 + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append([encoding[char] for char in next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n"
     ]
    }
   ],
   "source": [
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n",
      "['i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print([decoding[idx] for idx in y_data[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data.\n",
    "X = F.one_hot(torch.tensor(X_data), num_classes=num_chars).to(torch.float)\n",
    "y = torch.tensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many2many model\n",
    "class mmRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the number of h layers and the nodes in each layer\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        #out = out[:, -1] # as you can see this line was deleted: This means instead of taking just the last hidden\n",
    "        #state with out = out[:, -1] (size: batch_size x num_classes), we keep all the hidden states because out has \n",
    "        #a size of: batch_size x seq_len x num_classes (one hidden state per character in the sequence).\n",
    "\n",
    "        \n",
    "        #During inference, we take only the last hidden state\n",
    "        #because other predictions are used only to train the model\n",
    "        if not self.train:\n",
    "            out = out[:, -1]\n",
    "            \n",
    "        # the Linear layer works automatically with one more dimension.\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "mmRNNModel                               --                        --\n",
       "├─RNN: 1-1                               [128, 20, 256]            82,688\n",
       "├─Linear: 1-2                            [128, 20, 65]             16,705\n",
       "==========================================================================================\n",
       "Total params: 99,393\n",
       "Trainable params: 99,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 213.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.67\n",
       "Forward/backward pass size (MB): 6.57\n",
       "Params size (MB): 0.40\n",
       "Estimated Total Size (MB): 7.64\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "seq_length = 20\n",
    "num_classes = 65\n",
    "layer_size = 1\n",
    "batch_size= 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelmm = mmRNNModel(num_classes, hidden_size, layer_size, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(modelmm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(modelmm, input_size=(batch_size, seq_length, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, log_interval=200):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_correct = 0 \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(data_loader, desc=f\"Training Epoch {epoch}\")):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        #the only modification here consists in modifying the cross-entropy, because it takes inputs with \n",
    "        #dimensions: (N, C, d1, d2, ..., dk), that means the dimension of the classes is always the 2nd, \n",
    "        #and all the extra dimensions come at the end. You have to change your output to size:\n",
    "        # batch_size x seq_len x num_classes -> batch_size x num_classes x seq_len. \n",
    "        #It's just a transpose of the loss criterion (output.transpose(1, 2), target).\n",
    "        output = output.transpose(1, 2) \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, dim=1)\n",
    "\n",
    "        total_correct += torch.sum(pred == target).item()\n",
    "                  \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
    "    \n",
    "    accuracy_train = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    \n",
    "    total_train_loss = total_train_loss / len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_train_loss,\n",
    "        \"accuracy\": accuracy_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() \n",
    "def predict(model, data):\n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from X,y tensors\n",
    "datasett = TensorDataset(X, y)\n",
    "train_loader = DataLoader(datasett, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dc7217a1794c2ba3d66305dd059698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1115374 (0%)]\tLoss: 4.172177\n",
      "Train Epoch: 1 [25600/1115374 (2%)]\tLoss: 2.544350\n",
      "Train Epoch: 1 [51200/1115374 (5%)]\tLoss: 2.233426\n",
      "Train Epoch: 1 [76800/1115374 (7%)]\tLoss: 2.088389\n",
      "Train Epoch: 1 [102400/1115374 (9%)]\tLoss: 2.093889\n",
      "Train Epoch: 1 [128000/1115374 (11%)]\tLoss: 2.030957\n",
      "Train Epoch: 1 [153600/1115374 (14%)]\tLoss: 1.971463\n",
      "Train Epoch: 1 [179200/1115374 (16%)]\tLoss: 1.950469\n",
      "Train Epoch: 1 [204800/1115374 (18%)]\tLoss: 1.923469\n",
      "Train Epoch: 1 [230400/1115374 (21%)]\tLoss: 1.884498\n",
      "Train Epoch: 1 [256000/1115374 (23%)]\tLoss: 1.864313\n",
      "Train Epoch: 1 [281600/1115374 (25%)]\tLoss: 1.828224\n",
      "Train Epoch: 1 [307200/1115374 (28%)]\tLoss: 1.817915\n",
      "Train Epoch: 1 [332800/1115374 (30%)]\tLoss: 1.736223\n",
      "Train Epoch: 1 [358400/1115374 (32%)]\tLoss: 1.711920\n",
      "Train Epoch: 1 [384000/1115374 (34%)]\tLoss: 1.743092\n",
      "Train Epoch: 1 [409600/1115374 (37%)]\tLoss: 1.754062\n",
      "Train Epoch: 1 [435200/1115374 (39%)]\tLoss: 1.767477\n",
      "Train Epoch: 1 [460800/1115374 (41%)]\tLoss: 1.722563\n",
      "Train Epoch: 1 [486400/1115374 (44%)]\tLoss: 1.702817\n",
      "Train Epoch: 1 [512000/1115374 (46%)]\tLoss: 1.718565\n",
      "Train Epoch: 1 [537600/1115374 (48%)]\tLoss: 1.713435\n",
      "Train Epoch: 1 [563200/1115374 (50%)]\tLoss: 1.678163\n",
      "Train Epoch: 1 [588800/1115374 (53%)]\tLoss: 1.742047\n",
      "Train Epoch: 1 [614400/1115374 (55%)]\tLoss: 1.659556\n",
      "Train Epoch: 1 [640000/1115374 (57%)]\tLoss: 1.637041\n",
      "Train Epoch: 1 [665600/1115374 (60%)]\tLoss: 1.627295\n",
      "Train Epoch: 1 [691200/1115374 (62%)]\tLoss: 1.713288\n",
      "Train Epoch: 1 [716800/1115374 (64%)]\tLoss: 1.715913\n",
      "Train Epoch: 1 [742400/1115374 (67%)]\tLoss: 1.636680\n",
      "Train Epoch: 1 [768000/1115374 (69%)]\tLoss: 1.621563\n",
      "Train Epoch: 1 [793600/1115374 (71%)]\tLoss: 1.577881\n",
      "Train Epoch: 1 [819200/1115374 (73%)]\tLoss: 1.613419\n",
      "Train Epoch: 1 [844800/1115374 (76%)]\tLoss: 1.614644\n",
      "Train Epoch: 1 [870400/1115374 (78%)]\tLoss: 1.598505\n",
      "Train Epoch: 1 [896000/1115374 (80%)]\tLoss: 1.607293\n",
      "Train Epoch: 1 [921600/1115374 (83%)]\tLoss: 1.610091\n",
      "Train Epoch: 1 [947200/1115374 (85%)]\tLoss: 1.659287\n",
      "Train Epoch: 1 [972800/1115374 (87%)]\tLoss: 1.604076\n",
      "Train Epoch: 1 [998400/1115374 (90%)]\tLoss: 1.522610\n",
      "Train Epoch: 1 [1024000/1115374 (92%)]\tLoss: 1.511707\n",
      "Train Epoch: 1 [1049600/1115374 (94%)]\tLoss: 1.563700\n",
      "Train Epoch: 1 [1075200/1115374 (96%)]\tLoss: 1.622288\n",
      "Train Epoch: 1 [1100800/1115374 (99%)]\tLoss: 1.566473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce366bde051a49ba8e2b67b0139367c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/1115374 (0%)]\tLoss: 1.536698\n",
      "Train Epoch: 2 [25600/1115374 (2%)]\tLoss: 1.569369\n",
      "Train Epoch: 2 [51200/1115374 (5%)]\tLoss: 1.588265\n",
      "Train Epoch: 2 [76800/1115374 (7%)]\tLoss: 1.587728\n",
      "Train Epoch: 2 [102400/1115374 (9%)]\tLoss: 1.563061\n",
      "Train Epoch: 2 [128000/1115374 (11%)]\tLoss: 1.554513\n",
      "Train Epoch: 2 [153600/1115374 (14%)]\tLoss: 1.564109\n",
      "Train Epoch: 2 [179200/1115374 (16%)]\tLoss: 1.569998\n",
      "Train Epoch: 2 [204800/1115374 (18%)]\tLoss: 1.608607\n",
      "Train Epoch: 2 [230400/1115374 (21%)]\tLoss: 1.558463\n",
      "Train Epoch: 2 [256000/1115374 (23%)]\tLoss: 1.547767\n",
      "Train Epoch: 2 [281600/1115374 (25%)]\tLoss: 1.553298\n",
      "Train Epoch: 2 [307200/1115374 (28%)]\tLoss: 1.517699\n",
      "Train Epoch: 2 [332800/1115374 (30%)]\tLoss: 1.587879\n",
      "Train Epoch: 2 [358400/1115374 (32%)]\tLoss: 1.592025\n",
      "Train Epoch: 2 [384000/1115374 (34%)]\tLoss: 1.552779\n",
      "Train Epoch: 2 [409600/1115374 (37%)]\tLoss: 1.541762\n",
      "Train Epoch: 2 [435200/1115374 (39%)]\tLoss: 1.551604\n",
      "Train Epoch: 2 [460800/1115374 (41%)]\tLoss: 1.579680\n",
      "Train Epoch: 2 [486400/1115374 (44%)]\tLoss: 1.533309\n",
      "Train Epoch: 2 [512000/1115374 (46%)]\tLoss: 1.601537\n",
      "Train Epoch: 2 [537600/1115374 (48%)]\tLoss: 1.517098\n",
      "Train Epoch: 2 [563200/1115374 (50%)]\tLoss: 1.545440\n",
      "Train Epoch: 2 [588800/1115374 (53%)]\tLoss: 1.477999\n",
      "Train Epoch: 2 [614400/1115374 (55%)]\tLoss: 1.559029\n",
      "Train Epoch: 2 [640000/1115374 (57%)]\tLoss: 1.553525\n",
      "Train Epoch: 2 [665600/1115374 (60%)]\tLoss: 1.537999\n",
      "Train Epoch: 2 [691200/1115374 (62%)]\tLoss: 1.572468\n",
      "Train Epoch: 2 [716800/1115374 (64%)]\tLoss: 1.547580\n",
      "Train Epoch: 2 [742400/1115374 (67%)]\tLoss: 1.534140\n",
      "Train Epoch: 2 [768000/1115374 (69%)]\tLoss: 1.529367\n",
      "Train Epoch: 2 [793600/1115374 (71%)]\tLoss: 1.532044\n",
      "Train Epoch: 2 [819200/1115374 (73%)]\tLoss: 1.528825\n",
      "Train Epoch: 2 [844800/1115374 (76%)]\tLoss: 1.625347\n",
      "Train Epoch: 2 [870400/1115374 (78%)]\tLoss: 1.505432\n",
      "Train Epoch: 2 [896000/1115374 (80%)]\tLoss: 1.525162\n",
      "Train Epoch: 2 [921600/1115374 (83%)]\tLoss: 1.516687\n",
      "Train Epoch: 2 [947200/1115374 (85%)]\tLoss: 1.537430\n",
      "Train Epoch: 2 [972800/1115374 (87%)]\tLoss: 1.541378\n",
      "Train Epoch: 2 [998400/1115374 (90%)]\tLoss: 1.567547\n",
      "Train Epoch: 2 [1024000/1115374 (92%)]\tLoss: 1.574924\n",
      "Train Epoch: 2 [1049600/1115374 (94%)]\tLoss: 1.510939\n",
      "Train Epoch: 2 [1075200/1115374 (96%)]\tLoss: 1.549513\n",
      "Train Epoch: 2 [1100800/1115374 (99%)]\tLoss: 1.524175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4398a81ea93f49eb93f15ffa9b7000bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/1115374 (0%)]\tLoss: 1.539454\n",
      "Train Epoch: 3 [25600/1115374 (2%)]\tLoss: 1.504982\n",
      "Train Epoch: 3 [51200/1115374 (5%)]\tLoss: 1.529754\n",
      "Train Epoch: 3 [76800/1115374 (7%)]\tLoss: 1.518779\n",
      "Train Epoch: 3 [102400/1115374 (9%)]\tLoss: 1.512467\n",
      "Train Epoch: 3 [128000/1115374 (11%)]\tLoss: 1.484903\n",
      "Train Epoch: 3 [153600/1115374 (14%)]\tLoss: 1.470957\n",
      "Train Epoch: 3 [179200/1115374 (16%)]\tLoss: 1.526166\n",
      "Train Epoch: 3 [204800/1115374 (18%)]\tLoss: 1.515310\n",
      "Train Epoch: 3 [230400/1115374 (21%)]\tLoss: 1.527707\n",
      "Train Epoch: 3 [256000/1115374 (23%)]\tLoss: 1.516012\n",
      "Train Epoch: 3 [281600/1115374 (25%)]\tLoss: 1.533146\n",
      "Train Epoch: 3 [307200/1115374 (28%)]\tLoss: 1.525366\n",
      "Train Epoch: 3 [332800/1115374 (30%)]\tLoss: 1.495656\n",
      "Train Epoch: 3 [358400/1115374 (32%)]\tLoss: 1.561352\n",
      "Train Epoch: 3 [384000/1115374 (34%)]\tLoss: 1.526683\n",
      "Train Epoch: 3 [409600/1115374 (37%)]\tLoss: 1.531038\n",
      "Train Epoch: 3 [435200/1115374 (39%)]\tLoss: 1.534438\n",
      "Train Epoch: 3 [460800/1115374 (41%)]\tLoss: 1.498658\n",
      "Train Epoch: 3 [486400/1115374 (44%)]\tLoss: 1.485160\n",
      "Train Epoch: 3 [512000/1115374 (46%)]\tLoss: 1.499262\n",
      "Train Epoch: 3 [537600/1115374 (48%)]\tLoss: 1.470828\n",
      "Train Epoch: 3 [563200/1115374 (50%)]\tLoss: 1.504914\n",
      "Train Epoch: 3 [588800/1115374 (53%)]\tLoss: 1.523214\n",
      "Train Epoch: 3 [614400/1115374 (55%)]\tLoss: 1.421133\n",
      "Train Epoch: 3 [640000/1115374 (57%)]\tLoss: 1.526392\n",
      "Train Epoch: 3 [665600/1115374 (60%)]\tLoss: 1.573068\n",
      "Train Epoch: 3 [691200/1115374 (62%)]\tLoss: 1.491848\n",
      "Train Epoch: 3 [716800/1115374 (64%)]\tLoss: 1.481472\n",
      "Train Epoch: 3 [742400/1115374 (67%)]\tLoss: 1.518303\n",
      "Train Epoch: 3 [768000/1115374 (69%)]\tLoss: 1.491001\n",
      "Train Epoch: 3 [793600/1115374 (71%)]\tLoss: 1.514973\n",
      "Train Epoch: 3 [819200/1115374 (73%)]\tLoss: 1.503780\n",
      "Train Epoch: 3 [844800/1115374 (76%)]\tLoss: 1.500012\n",
      "Train Epoch: 3 [870400/1115374 (78%)]\tLoss: 1.533174\n",
      "Train Epoch: 3 [896000/1115374 (80%)]\tLoss: 1.472688\n",
      "Train Epoch: 3 [921600/1115374 (83%)]\tLoss: 1.531636\n",
      "Train Epoch: 3 [947200/1115374 (85%)]\tLoss: 1.503141\n",
      "Train Epoch: 3 [972800/1115374 (87%)]\tLoss: 1.493779\n",
      "Train Epoch: 3 [998400/1115374 (90%)]\tLoss: 1.470555\n",
      "Train Epoch: 3 [1024000/1115374 (92%)]\tLoss: 1.496726\n",
      "Train Epoch: 3 [1049600/1115374 (94%)]\tLoss: 1.516667\n",
      "Train Epoch: 3 [1075200/1115374 (96%)]\tLoss: 1.510700\n",
      "Train Epoch: 3 [1100800/1115374 (99%)]\tLoss: 1.538691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c6c9490e164b20a234895ff0182557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/1115374 (0%)]\tLoss: 1.509487\n",
      "Train Epoch: 4 [25600/1115374 (2%)]\tLoss: 1.458155\n",
      "Train Epoch: 4 [51200/1115374 (5%)]\tLoss: 1.523409\n",
      "Train Epoch: 4 [76800/1115374 (7%)]\tLoss: 1.476128\n",
      "Train Epoch: 4 [102400/1115374 (9%)]\tLoss: 1.466203\n",
      "Train Epoch: 4 [128000/1115374 (11%)]\tLoss: 1.473978\n",
      "Train Epoch: 4 [153600/1115374 (14%)]\tLoss: 1.531862\n",
      "Train Epoch: 4 [179200/1115374 (16%)]\tLoss: 1.470111\n",
      "Train Epoch: 4 [204800/1115374 (18%)]\tLoss: 1.447136\n",
      "Train Epoch: 4 [230400/1115374 (21%)]\tLoss: 1.481505\n",
      "Train Epoch: 4 [256000/1115374 (23%)]\tLoss: 1.501323\n",
      "Train Epoch: 4 [281600/1115374 (25%)]\tLoss: 1.530830\n",
      "Train Epoch: 4 [307200/1115374 (28%)]\tLoss: 1.525326\n",
      "Train Epoch: 4 [332800/1115374 (30%)]\tLoss: 1.508922\n",
      "Train Epoch: 4 [358400/1115374 (32%)]\tLoss: 1.518891\n",
      "Train Epoch: 4 [384000/1115374 (34%)]\tLoss: 1.500841\n",
      "Train Epoch: 4 [409600/1115374 (37%)]\tLoss: 1.539945\n",
      "Train Epoch: 4 [435200/1115374 (39%)]\tLoss: 1.486008\n",
      "Train Epoch: 4 [460800/1115374 (41%)]\tLoss: 1.498008\n",
      "Train Epoch: 4 [486400/1115374 (44%)]\tLoss: 1.472452\n",
      "Train Epoch: 4 [512000/1115374 (46%)]\tLoss: 1.474781\n",
      "Train Epoch: 4 [537600/1115374 (48%)]\tLoss: 1.478297\n",
      "Train Epoch: 4 [563200/1115374 (50%)]\tLoss: 1.492854\n",
      "Train Epoch: 4 [588800/1115374 (53%)]\tLoss: 1.458335\n",
      "Train Epoch: 4 [614400/1115374 (55%)]\tLoss: 1.526650\n",
      "Train Epoch: 4 [640000/1115374 (57%)]\tLoss: 1.575663\n",
      "Train Epoch: 4 [665600/1115374 (60%)]\tLoss: 1.453042\n",
      "Train Epoch: 4 [691200/1115374 (62%)]\tLoss: 1.448340\n",
      "Train Epoch: 4 [716800/1115374 (64%)]\tLoss: 1.471894\n",
      "Train Epoch: 4 [742400/1115374 (67%)]\tLoss: 1.508975\n",
      "Train Epoch: 4 [768000/1115374 (69%)]\tLoss: 1.508686\n",
      "Train Epoch: 4 [793600/1115374 (71%)]\tLoss: 1.476904\n",
      "Train Epoch: 4 [819200/1115374 (73%)]\tLoss: 1.483311\n",
      "Train Epoch: 4 [844800/1115374 (76%)]\tLoss: 1.556363\n",
      "Train Epoch: 4 [870400/1115374 (78%)]\tLoss: 1.553830\n",
      "Train Epoch: 4 [896000/1115374 (80%)]\tLoss: 1.501948\n",
      "Train Epoch: 4 [921600/1115374 (83%)]\tLoss: 1.508304\n",
      "Train Epoch: 4 [947200/1115374 (85%)]\tLoss: 1.414902\n",
      "Train Epoch: 4 [972800/1115374 (87%)]\tLoss: 1.425732\n",
      "Train Epoch: 4 [998400/1115374 (90%)]\tLoss: 1.472522\n",
      "Train Epoch: 4 [1024000/1115374 (92%)]\tLoss: 1.518438\n",
      "Train Epoch: 4 [1049600/1115374 (94%)]\tLoss: 1.478074\n",
      "Train Epoch: 4 [1075200/1115374 (96%)]\tLoss: 1.479930\n",
      "Train Epoch: 4 [1100800/1115374 (99%)]\tLoss: 1.435904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a097f50f0b4cde9d9d4982e6cee9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/1115374 (0%)]\tLoss: 1.396901\n",
      "Train Epoch: 5 [25600/1115374 (2%)]\tLoss: 1.472029\n",
      "Train Epoch: 5 [51200/1115374 (5%)]\tLoss: 1.564534\n",
      "Train Epoch: 5 [76800/1115374 (7%)]\tLoss: 1.464843\n",
      "Train Epoch: 5 [102400/1115374 (9%)]\tLoss: 1.471632\n",
      "Train Epoch: 5 [128000/1115374 (11%)]\tLoss: 1.447609\n",
      "Train Epoch: 5 [153600/1115374 (14%)]\tLoss: 1.465453\n",
      "Train Epoch: 5 [179200/1115374 (16%)]\tLoss: 1.504221\n",
      "Train Epoch: 5 [204800/1115374 (18%)]\tLoss: 1.491246\n",
      "Train Epoch: 5 [230400/1115374 (21%)]\tLoss: 1.449474\n",
      "Train Epoch: 5 [256000/1115374 (23%)]\tLoss: 1.462004\n",
      "Train Epoch: 5 [281600/1115374 (25%)]\tLoss: 1.463349\n",
      "Train Epoch: 5 [307200/1115374 (28%)]\tLoss: 1.461627\n",
      "Train Epoch: 5 [332800/1115374 (30%)]\tLoss: 1.502609\n",
      "Train Epoch: 5 [358400/1115374 (32%)]\tLoss: 1.510509\n",
      "Train Epoch: 5 [384000/1115374 (34%)]\tLoss: 1.460523\n",
      "Train Epoch: 5 [409600/1115374 (37%)]\tLoss: 1.463245\n",
      "Train Epoch: 5 [435200/1115374 (39%)]\tLoss: 1.479582\n",
      "Train Epoch: 5 [460800/1115374 (41%)]\tLoss: 1.491033\n",
      "Train Epoch: 5 [486400/1115374 (44%)]\tLoss: 1.437676\n",
      "Train Epoch: 5 [512000/1115374 (46%)]\tLoss: 1.484835\n",
      "Train Epoch: 5 [537600/1115374 (48%)]\tLoss: 1.460322\n",
      "Train Epoch: 5 [563200/1115374 (50%)]\tLoss: 1.514116\n",
      "Train Epoch: 5 [588800/1115374 (53%)]\tLoss: 1.457910\n",
      "Train Epoch: 5 [614400/1115374 (55%)]\tLoss: 1.516675\n",
      "Train Epoch: 5 [640000/1115374 (57%)]\tLoss: 1.485370\n",
      "Train Epoch: 5 [665600/1115374 (60%)]\tLoss: 1.444726\n",
      "Train Epoch: 5 [691200/1115374 (62%)]\tLoss: 1.478731\n",
      "Train Epoch: 5 [716800/1115374 (64%)]\tLoss: 1.462720\n",
      "Train Epoch: 5 [742400/1115374 (67%)]\tLoss: 1.511688\n",
      "Train Epoch: 5 [768000/1115374 (69%)]\tLoss: 1.488659\n",
      "Train Epoch: 5 [793600/1115374 (71%)]\tLoss: 1.521250\n",
      "Train Epoch: 5 [819200/1115374 (73%)]\tLoss: 1.486764\n",
      "Train Epoch: 5 [844800/1115374 (76%)]\tLoss: 1.515293\n",
      "Train Epoch: 5 [870400/1115374 (78%)]\tLoss: 1.434067\n",
      "Train Epoch: 5 [896000/1115374 (80%)]\tLoss: 1.501543\n",
      "Train Epoch: 5 [921600/1115374 (83%)]\tLoss: 1.477628\n",
      "Train Epoch: 5 [947200/1115374 (85%)]\tLoss: 1.499131\n",
      "Train Epoch: 5 [972800/1115374 (87%)]\tLoss: 1.477966\n",
      "Train Epoch: 5 [998400/1115374 (90%)]\tLoss: 1.497991\n",
      "Train Epoch: 5 [1024000/1115374 (92%)]\tLoss: 1.501472\n",
      "Train Epoch: 5 [1049600/1115374 (94%)]\tLoss: 1.440531\n",
      "Train Epoch: 5 [1075200/1115374 (96%)]\tLoss: 1.461797\n",
      "Train Epoch: 5 [1100800/1115374 (99%)]\tLoss: 1.454492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90d5d42f23d4a328402b4731c7e60e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/1115374 (0%)]\tLoss: 1.477747\n",
      "Train Epoch: 6 [25600/1115374 (2%)]\tLoss: 1.495365\n",
      "Train Epoch: 6 [51200/1115374 (5%)]\tLoss: 1.442849\n",
      "Train Epoch: 6 [76800/1115374 (7%)]\tLoss: 1.472844\n",
      "Train Epoch: 6 [102400/1115374 (9%)]\tLoss: 1.455407\n",
      "Train Epoch: 6 [128000/1115374 (11%)]\tLoss: 1.513244\n",
      "Train Epoch: 6 [153600/1115374 (14%)]\tLoss: 1.474970\n",
      "Train Epoch: 6 [179200/1115374 (16%)]\tLoss: 1.447728\n",
      "Train Epoch: 6 [204800/1115374 (18%)]\tLoss: 1.443928\n",
      "Train Epoch: 6 [230400/1115374 (21%)]\tLoss: 1.488297\n",
      "Train Epoch: 6 [256000/1115374 (23%)]\tLoss: 1.417107\n",
      "Train Epoch: 6 [281600/1115374 (25%)]\tLoss: 1.464979\n",
      "Train Epoch: 6 [307200/1115374 (28%)]\tLoss: 1.517345\n",
      "Train Epoch: 6 [332800/1115374 (30%)]\tLoss: 1.466341\n",
      "Train Epoch: 6 [358400/1115374 (32%)]\tLoss: 1.452345\n",
      "Train Epoch: 6 [384000/1115374 (34%)]\tLoss: 1.479474\n",
      "Train Epoch: 6 [409600/1115374 (37%)]\tLoss: 1.489272\n",
      "Train Epoch: 6 [435200/1115374 (39%)]\tLoss: 1.517976\n",
      "Train Epoch: 6 [460800/1115374 (41%)]\tLoss: 1.453275\n",
      "Train Epoch: 6 [486400/1115374 (44%)]\tLoss: 1.423764\n",
      "Train Epoch: 6 [512000/1115374 (46%)]\tLoss: 1.450188\n",
      "Train Epoch: 6 [537600/1115374 (48%)]\tLoss: 1.460539\n",
      "Train Epoch: 6 [563200/1115374 (50%)]\tLoss: 1.463769\n",
      "Train Epoch: 6 [588800/1115374 (53%)]\tLoss: 1.458011\n",
      "Train Epoch: 6 [614400/1115374 (55%)]\tLoss: 1.462814\n",
      "Train Epoch: 6 [640000/1115374 (57%)]\tLoss: 1.467906\n",
      "Train Epoch: 6 [665600/1115374 (60%)]\tLoss: 1.419646\n",
      "Train Epoch: 6 [691200/1115374 (62%)]\tLoss: 1.430097\n",
      "Train Epoch: 6 [716800/1115374 (64%)]\tLoss: 1.433466\n",
      "Train Epoch: 6 [742400/1115374 (67%)]\tLoss: 1.467058\n",
      "Train Epoch: 6 [768000/1115374 (69%)]\tLoss: 1.445907\n",
      "Train Epoch: 6 [793600/1115374 (71%)]\tLoss: 1.509182\n",
      "Train Epoch: 6 [819200/1115374 (73%)]\tLoss: 1.441646\n",
      "Train Epoch: 6 [844800/1115374 (76%)]\tLoss: 1.494821\n",
      "Train Epoch: 6 [870400/1115374 (78%)]\tLoss: 1.503986\n",
      "Train Epoch: 6 [896000/1115374 (80%)]\tLoss: 1.473492\n",
      "Train Epoch: 6 [921600/1115374 (83%)]\tLoss: 1.530211\n",
      "Train Epoch: 6 [947200/1115374 (85%)]\tLoss: 1.434288\n",
      "Train Epoch: 6 [972800/1115374 (87%)]\tLoss: 1.448491\n",
      "Train Epoch: 6 [998400/1115374 (90%)]\tLoss: 1.423240\n",
      "Train Epoch: 6 [1024000/1115374 (92%)]\tLoss: 1.467148\n",
      "Train Epoch: 6 [1049600/1115374 (94%)]\tLoss: 1.483970\n",
      "Train Epoch: 6 [1075200/1115374 (96%)]\tLoss: 1.475513\n",
      "Train Epoch: 6 [1100800/1115374 (99%)]\tLoss: 1.463122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb83de1356042d5964bfb492b200025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/1115374 (0%)]\tLoss: 1.458042\n",
      "Train Epoch: 7 [25600/1115374 (2%)]\tLoss: 1.505295\n",
      "Train Epoch: 7 [51200/1115374 (5%)]\tLoss: 1.487973\n",
      "Train Epoch: 7 [76800/1115374 (7%)]\tLoss: 1.450532\n",
      "Train Epoch: 7 [102400/1115374 (9%)]\tLoss: 1.519175\n",
      "Train Epoch: 7 [128000/1115374 (11%)]\tLoss: 1.447730\n",
      "Train Epoch: 7 [153600/1115374 (14%)]\tLoss: 1.385994\n",
      "Train Epoch: 7 [179200/1115374 (16%)]\tLoss: 1.455643\n",
      "Train Epoch: 7 [204800/1115374 (18%)]\tLoss: 1.431857\n",
      "Train Epoch: 7 [230400/1115374 (21%)]\tLoss: 1.508147\n",
      "Train Epoch: 7 [256000/1115374 (23%)]\tLoss: 1.490445\n",
      "Train Epoch: 7 [281600/1115374 (25%)]\tLoss: 1.458014\n",
      "Train Epoch: 7 [307200/1115374 (28%)]\tLoss: 1.456334\n",
      "Train Epoch: 7 [332800/1115374 (30%)]\tLoss: 1.439624\n",
      "Train Epoch: 7 [358400/1115374 (32%)]\tLoss: 1.438722\n",
      "Train Epoch: 7 [384000/1115374 (34%)]\tLoss: 1.507646\n",
      "Train Epoch: 7 [409600/1115374 (37%)]\tLoss: 1.516918\n",
      "Train Epoch: 7 [435200/1115374 (39%)]\tLoss: 1.486762\n",
      "Train Epoch: 7 [460800/1115374 (41%)]\tLoss: 1.422819\n",
      "Train Epoch: 7 [486400/1115374 (44%)]\tLoss: 1.465782\n",
      "Train Epoch: 7 [512000/1115374 (46%)]\tLoss: 1.487692\n",
      "Train Epoch: 7 [537600/1115374 (48%)]\tLoss: 1.427302\n",
      "Train Epoch: 7 [563200/1115374 (50%)]\tLoss: 1.475469\n",
      "Train Epoch: 7 [588800/1115374 (53%)]\tLoss: 1.416759\n",
      "Train Epoch: 7 [614400/1115374 (55%)]\tLoss: 1.469768\n",
      "Train Epoch: 7 [640000/1115374 (57%)]\tLoss: 1.435179\n",
      "Train Epoch: 7 [665600/1115374 (60%)]\tLoss: 1.534303\n",
      "Train Epoch: 7 [691200/1115374 (62%)]\tLoss: 1.437838\n",
      "Train Epoch: 7 [716800/1115374 (64%)]\tLoss: 1.473480\n",
      "Train Epoch: 7 [742400/1115374 (67%)]\tLoss: 1.445138\n",
      "Train Epoch: 7 [768000/1115374 (69%)]\tLoss: 1.476663\n",
      "Train Epoch: 7 [793600/1115374 (71%)]\tLoss: 1.395319\n",
      "Train Epoch: 7 [819200/1115374 (73%)]\tLoss: 1.438384\n",
      "Train Epoch: 7 [844800/1115374 (76%)]\tLoss: 1.492942\n",
      "Train Epoch: 7 [870400/1115374 (78%)]\tLoss: 1.422236\n",
      "Train Epoch: 7 [896000/1115374 (80%)]\tLoss: 1.436942\n",
      "Train Epoch: 7 [921600/1115374 (83%)]\tLoss: 1.507779\n",
      "Train Epoch: 7 [947200/1115374 (85%)]\tLoss: 1.486897\n",
      "Train Epoch: 7 [972800/1115374 (87%)]\tLoss: 1.497970\n",
      "Train Epoch: 7 [998400/1115374 (90%)]\tLoss: 1.451367\n",
      "Train Epoch: 7 [1024000/1115374 (92%)]\tLoss: 1.420813\n",
      "Train Epoch: 7 [1049600/1115374 (94%)]\tLoss: 1.441002\n",
      "Train Epoch: 7 [1075200/1115374 (96%)]\tLoss: 1.457745\n",
      "Train Epoch: 7 [1100800/1115374 (99%)]\tLoss: 1.418947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db36c958eb77467fa5696fef011f1ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/1115374 (0%)]\tLoss: 1.501893\n",
      "Train Epoch: 8 [25600/1115374 (2%)]\tLoss: 1.496443\n",
      "Train Epoch: 8 [51200/1115374 (5%)]\tLoss: 1.450829\n",
      "Train Epoch: 8 [76800/1115374 (7%)]\tLoss: 1.478724\n",
      "Train Epoch: 8 [102400/1115374 (9%)]\tLoss: 1.395806\n",
      "Train Epoch: 8 [128000/1115374 (11%)]\tLoss: 1.430150\n",
      "Train Epoch: 8 [153600/1115374 (14%)]\tLoss: 1.437168\n",
      "Train Epoch: 8 [179200/1115374 (16%)]\tLoss: 1.479817\n",
      "Train Epoch: 8 [204800/1115374 (18%)]\tLoss: 1.469563\n",
      "Train Epoch: 8 [230400/1115374 (21%)]\tLoss: 1.448378\n",
      "Train Epoch: 8 [256000/1115374 (23%)]\tLoss: 1.436480\n",
      "Train Epoch: 8 [281600/1115374 (25%)]\tLoss: 1.424918\n",
      "Train Epoch: 8 [307200/1115374 (28%)]\tLoss: 1.464126\n",
      "Train Epoch: 8 [332800/1115374 (30%)]\tLoss: 1.479537\n",
      "Train Epoch: 8 [358400/1115374 (32%)]\tLoss: 1.473022\n",
      "Train Epoch: 8 [384000/1115374 (34%)]\tLoss: 1.481032\n",
      "Train Epoch: 8 [409600/1115374 (37%)]\tLoss: 1.422234\n",
      "Train Epoch: 8 [435200/1115374 (39%)]\tLoss: 1.468623\n",
      "Train Epoch: 8 [460800/1115374 (41%)]\tLoss: 1.452960\n",
      "Train Epoch: 8 [486400/1115374 (44%)]\tLoss: 1.467458\n",
      "Train Epoch: 8 [512000/1115374 (46%)]\tLoss: 1.436022\n",
      "Train Epoch: 8 [537600/1115374 (48%)]\tLoss: 1.434984\n",
      "Train Epoch: 8 [563200/1115374 (50%)]\tLoss: 1.442519\n",
      "Train Epoch: 8 [588800/1115374 (53%)]\tLoss: 1.490997\n",
      "Train Epoch: 8 [614400/1115374 (55%)]\tLoss: 1.483490\n",
      "Train Epoch: 8 [640000/1115374 (57%)]\tLoss: 1.448991\n",
      "Train Epoch: 8 [665600/1115374 (60%)]\tLoss: 1.443021\n",
      "Train Epoch: 8 [691200/1115374 (62%)]\tLoss: 1.499524\n",
      "Train Epoch: 8 [716800/1115374 (64%)]\tLoss: 1.475067\n",
      "Train Epoch: 8 [742400/1115374 (67%)]\tLoss: 1.424111\n",
      "Train Epoch: 8 [768000/1115374 (69%)]\tLoss: 1.464528\n",
      "Train Epoch: 8 [793600/1115374 (71%)]\tLoss: 1.407474\n",
      "Train Epoch: 8 [819200/1115374 (73%)]\tLoss: 1.398280\n",
      "Train Epoch: 8 [844800/1115374 (76%)]\tLoss: 1.471123\n",
      "Train Epoch: 8 [870400/1115374 (78%)]\tLoss: 1.467194\n",
      "Train Epoch: 8 [896000/1115374 (80%)]\tLoss: 1.452739\n",
      "Train Epoch: 8 [921600/1115374 (83%)]\tLoss: 1.496434\n",
      "Train Epoch: 8 [947200/1115374 (85%)]\tLoss: 1.441111\n",
      "Train Epoch: 8 [972800/1115374 (87%)]\tLoss: 1.459824\n",
      "Train Epoch: 8 [998400/1115374 (90%)]\tLoss: 1.425419\n",
      "Train Epoch: 8 [1024000/1115374 (92%)]\tLoss: 1.441763\n",
      "Train Epoch: 8 [1049600/1115374 (94%)]\tLoss: 1.487888\n",
      "Train Epoch: 8 [1075200/1115374 (96%)]\tLoss: 1.416043\n",
      "Train Epoch: 8 [1100800/1115374 (99%)]\tLoss: 1.467430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469da97767d344dba1039954098b014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/1115374 (0%)]\tLoss: 1.413422\n",
      "Train Epoch: 9 [25600/1115374 (2%)]\tLoss: 1.488958\n",
      "Train Epoch: 9 [51200/1115374 (5%)]\tLoss: 1.499605\n",
      "Train Epoch: 9 [76800/1115374 (7%)]\tLoss: 1.484985\n",
      "Train Epoch: 9 [102400/1115374 (9%)]\tLoss: 1.448737\n",
      "Train Epoch: 9 [128000/1115374 (11%)]\tLoss: 1.461100\n",
      "Train Epoch: 9 [153600/1115374 (14%)]\tLoss: 1.419195\n",
      "Train Epoch: 9 [179200/1115374 (16%)]\tLoss: 1.469285\n",
      "Train Epoch: 9 [204800/1115374 (18%)]\tLoss: 1.456138\n",
      "Train Epoch: 9 [230400/1115374 (21%)]\tLoss: 1.452205\n",
      "Train Epoch: 9 [256000/1115374 (23%)]\tLoss: 1.426126\n",
      "Train Epoch: 9 [281600/1115374 (25%)]\tLoss: 1.420633\n",
      "Train Epoch: 9 [307200/1115374 (28%)]\tLoss: 1.471583\n",
      "Train Epoch: 9 [332800/1115374 (30%)]\tLoss: 1.476816\n",
      "Train Epoch: 9 [358400/1115374 (32%)]\tLoss: 1.497860\n",
      "Train Epoch: 9 [384000/1115374 (34%)]\tLoss: 1.518155\n",
      "Train Epoch: 9 [409600/1115374 (37%)]\tLoss: 1.450316\n",
      "Train Epoch: 9 [435200/1115374 (39%)]\tLoss: 1.466191\n",
      "Train Epoch: 9 [460800/1115374 (41%)]\tLoss: 1.479591\n",
      "Train Epoch: 9 [486400/1115374 (44%)]\tLoss: 1.426466\n",
      "Train Epoch: 9 [512000/1115374 (46%)]\tLoss: 1.418297\n",
      "Train Epoch: 9 [537600/1115374 (48%)]\tLoss: 1.429575\n",
      "Train Epoch: 9 [563200/1115374 (50%)]\tLoss: 1.502835\n",
      "Train Epoch: 9 [588800/1115374 (53%)]\tLoss: 1.469111\n",
      "Train Epoch: 9 [614400/1115374 (55%)]\tLoss: 1.420828\n",
      "Train Epoch: 9 [640000/1115374 (57%)]\tLoss: 1.419250\n",
      "Train Epoch: 9 [665600/1115374 (60%)]\tLoss: 1.403242\n",
      "Train Epoch: 9 [691200/1115374 (62%)]\tLoss: 1.391512\n",
      "Train Epoch: 9 [716800/1115374 (64%)]\tLoss: 1.498649\n",
      "Train Epoch: 9 [742400/1115374 (67%)]\tLoss: 1.460079\n",
      "Train Epoch: 9 [768000/1115374 (69%)]\tLoss: 1.448150\n",
      "Train Epoch: 9 [793600/1115374 (71%)]\tLoss: 1.427986\n",
      "Train Epoch: 9 [819200/1115374 (73%)]\tLoss: 1.446872\n",
      "Train Epoch: 9 [844800/1115374 (76%)]\tLoss: 1.486021\n",
      "Train Epoch: 9 [870400/1115374 (78%)]\tLoss: 1.469147\n",
      "Train Epoch: 9 [896000/1115374 (80%)]\tLoss: 1.423284\n",
      "Train Epoch: 9 [921600/1115374 (83%)]\tLoss: 1.459876\n",
      "Train Epoch: 9 [947200/1115374 (85%)]\tLoss: 1.384783\n",
      "Train Epoch: 9 [972800/1115374 (87%)]\tLoss: 1.434539\n",
      "Train Epoch: 9 [998400/1115374 (90%)]\tLoss: 1.367823\n",
      "Train Epoch: 9 [1024000/1115374 (92%)]\tLoss: 1.470762\n",
      "Train Epoch: 9 [1049600/1115374 (94%)]\tLoss: 1.503571\n",
      "Train Epoch: 9 [1075200/1115374 (96%)]\tLoss: 1.450894\n",
      "Train Epoch: 9 [1100800/1115374 (99%)]\tLoss: 1.410057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db25517e956448ee914d1a4d5d7cc297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/8714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [0/1115374 (0%)]\tLoss: 1.434524\n",
      "Train Epoch: 10 [25600/1115374 (2%)]\tLoss: 1.475028\n",
      "Train Epoch: 10 [51200/1115374 (5%)]\tLoss: 1.410230\n",
      "Train Epoch: 10 [76800/1115374 (7%)]\tLoss: 1.415122\n",
      "Train Epoch: 10 [102400/1115374 (9%)]\tLoss: 1.391300\n",
      "Train Epoch: 10 [128000/1115374 (11%)]\tLoss: 1.477898\n",
      "Train Epoch: 10 [153600/1115374 (14%)]\tLoss: 1.475510\n",
      "Train Epoch: 10 [179200/1115374 (16%)]\tLoss: 1.444108\n",
      "Train Epoch: 10 [204800/1115374 (18%)]\tLoss: 1.461217\n",
      "Train Epoch: 10 [230400/1115374 (21%)]\tLoss: 1.455505\n",
      "Train Epoch: 10 [256000/1115374 (23%)]\tLoss: 1.439392\n",
      "Train Epoch: 10 [281600/1115374 (25%)]\tLoss: 1.430392\n",
      "Train Epoch: 10 [307200/1115374 (28%)]\tLoss: 1.508476\n",
      "Train Epoch: 10 [332800/1115374 (30%)]\tLoss: 1.406947\n",
      "Train Epoch: 10 [358400/1115374 (32%)]\tLoss: 1.451527\n",
      "Train Epoch: 10 [384000/1115374 (34%)]\tLoss: 1.459649\n",
      "Train Epoch: 10 [409600/1115374 (37%)]\tLoss: 1.451418\n",
      "Train Epoch: 10 [435200/1115374 (39%)]\tLoss: 1.448162\n",
      "Train Epoch: 10 [460800/1115374 (41%)]\tLoss: 1.434488\n",
      "Train Epoch: 10 [486400/1115374 (44%)]\tLoss: 1.469977\n",
      "Train Epoch: 10 [512000/1115374 (46%)]\tLoss: 1.490957\n",
      "Train Epoch: 10 [537600/1115374 (48%)]\tLoss: 1.511632\n",
      "Train Epoch: 10 [563200/1115374 (50%)]\tLoss: 1.430611\n",
      "Train Epoch: 10 [588800/1115374 (53%)]\tLoss: 1.449369\n",
      "Train Epoch: 10 [614400/1115374 (55%)]\tLoss: 1.473430\n",
      "Train Epoch: 10 [640000/1115374 (57%)]\tLoss: 1.396227\n",
      "Train Epoch: 10 [665600/1115374 (60%)]\tLoss: 1.441802\n",
      "Train Epoch: 10 [691200/1115374 (62%)]\tLoss: 1.429743\n",
      "Train Epoch: 10 [716800/1115374 (64%)]\tLoss: 1.434426\n",
      "Train Epoch: 10 [742400/1115374 (67%)]\tLoss: 1.434520\n",
      "Train Epoch: 10 [768000/1115374 (69%)]\tLoss: 1.483518\n",
      "Train Epoch: 10 [793600/1115374 (71%)]\tLoss: 1.392015\n",
      "Train Epoch: 10 [819200/1115374 (73%)]\tLoss: 1.400261\n",
      "Train Epoch: 10 [844800/1115374 (76%)]\tLoss: 1.443609\n",
      "Train Epoch: 10 [870400/1115374 (78%)]\tLoss: 1.430792\n",
      "Train Epoch: 10 [896000/1115374 (80%)]\tLoss: 1.401811\n",
      "Train Epoch: 10 [921600/1115374 (83%)]\tLoss: 1.432697\n",
      "Train Epoch: 10 [947200/1115374 (85%)]\tLoss: 1.426106\n",
      "Train Epoch: 10 [972800/1115374 (87%)]\tLoss: 1.398462\n",
      "Train Epoch: 10 [998400/1115374 (90%)]\tLoss: 1.421353\n",
      "Train Epoch: 10 [1024000/1115374 (92%)]\tLoss: 1.427849\n",
      "Train Epoch: 10 [1049600/1115374 (94%)]\tLoss: 1.438199\n",
      "Train Epoch: 10 [1075200/1115374 (96%)]\tLoss: 1.482077\n",
      "Train Epoch: 10 [1100800/1115374 (99%)]\tLoss: 1.442235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "# Keep track of stats to plot them\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_result = train(epoch, modelmm, train_loader)\n",
    "    train_losses.append(train_result[\"loss\"])\n",
    "    train_accuracies.append(train_result[\"accuracy\"])\n",
    "    \n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is loud and first, for ever do spoke and some purson in man go divito cqure of the maid the nupheling the Tiren\n",
      "Herefor fame!\n",
      "He'll at need flesh:\n",
      "I though an own shep\n",
      "him not hear, though it have becomes hide?\n",
      "Think your holy her;\n",
      "We was a crow: I did kiss and were course?\n",
      "Doou may I spoke anot bring. Come, fair 'nighter'd him.\n",
      "\n",
      "PROSPERO:\n",
      "I will this how you,\n",
      "When tributy,--Sound, conded at the birn's reasour expeach,\n",
      "Though now to juit twice done a man, both duke\n",
      "I saw bring he succe,\n",
      "Her hav\n"
     ]
    }
   ],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  \n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            \n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"In the early morning, the flower is shining\")\n",
    "\n",
    "# To Do as an optimization: add function here that takes as parameters the seed_pattern and the model\n",
    "\n",
    "encoded_text = torch.tensor([encoding[char] for char in seed_pattern])\n",
    "encoded_text = F.one_hot(encoded_text, num_classes=num_chars).to(torch.float)\n",
    "# Add a single batch dimension at the beginning\n",
    "encoded_text = encoded_text.unsqueeze(0)\n",
    "encoded_text = encoded_text.to(device)\n",
    "torch.manual_seed(42)\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output = predict(modelmm, encoded_text)[0]\n",
    "    # Convert the output to probabilities\n",
    "    probs = torch.softmax(output[-1], dim=-1)\n",
    "    # make the generation more diverse.\n",
    "    prediction = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    generated_text += decoding[int(prediction)]\n",
    "    # One hot encode the new (predicted) character\n",
    "    next_char_encoded = F.one_hot(prediction, num_classes=num_chars)\n",
    "    # Make sure it has a singular batch and seq_len dimension in order to concatenate them.\n",
    "    next_char_encoded = next_char_encoded.view(1, 1, num_chars)\n",
    "    # Remove first char and glue the predicted one to the end\n",
    "    encoded_text = torch.cat((encoded_text[:, 1:], next_char_encoded), dim=1)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
