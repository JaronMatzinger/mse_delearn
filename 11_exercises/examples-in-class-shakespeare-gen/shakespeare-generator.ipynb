{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras = tf.keras\n",
    "# wget the file from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
    "with open(\"shakespeare.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "    corpus_length = len(corpus)\n",
    "print(\"Loaded a corpus of {0} characters\".format(corpus_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a unique identifier for each char in the corpus, \n",
    "# then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c:i for i, c in enumerate(chars)}\n",
    "decoding = {i:c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(encoding)\n",
    "print(decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One to many approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Encoded data[0]          : {}'.format(X_data[0]))\n",
    "print('Target of data[0]        : {}'.format(y_data[0]))\n",
    "print('Decoded data[0]          : {}'.format([decoding[idx] for idx in X_data[0]]))\n",
    "print('Decoded Target of data[0]: {}'.format(decoding[y_data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorize our data and labels. We want everything in one-hot.\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\"\n",
    "      .format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\"\n",
    "      .format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define our model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(shape=(sentence_length, num_chars)))\n",
    "model.add(keras.layers.SimpleRNN(256, return_sequences=False))\n",
    "model.add(keras.layers.Dense(num_chars))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training\n",
    "log = model.fit(X, y, epochs=10, batch_size=128)\n",
    "plt.plot(log.history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"Once upon a time in \")\n",
    "print(\"seed = \" + seed_pattern)\n",
    "\n",
    "X = np.zeros((1, sentence_length, num_chars), dtype=float)\n",
    "for i, character in enumerate(seed_pattern):\n",
    "    X[0, i, encoding[character]] = 1\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output_prob = model.predict(X, verbose=0)[0]\n",
    "    # in previous line predict() gives a tensor of shape (1, 65) \n",
    "    # with 1 being the size of the batch, for that we use [0] to get a vector\n",
    "    prediction = np.random.choice(num_chars, p = output_prob )\n",
    "    generated_text += decoding[prediction]\n",
    "    activations = np.zeros((1, 1, num_chars), dtype=bool)\n",
    "    activations[0, 0, prediction] = 1\n",
    "    #now remove first char and glue the predicted one\n",
    "    X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop up our data into X and y, slice into roughly \n",
    "# (num_chars / skip) overlapping 'sentences' of length \n",
    "# sentence_length, and encode the chars\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i+1:i+1 + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append([encoding[char] for char in next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\"\n",
    "      .format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([decoding[idx] for idx in X_data[0]])\n",
    "print([decoding[idx] for idx in y_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize our data and labels. We want everything in one-hot.\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "for i, sentence in enumerate(y_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        y[i, t, encoded_char] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "modelmm = keras.models.Sequential()\n",
    "modelmm.add(keras.layers.InputLayer(shape=(sentence_length, num_chars)))\n",
    "modelmm.add(keras.layers.SimpleRNN(256, return_sequences=True))\n",
    "modelmm.add(keras.layers.TimeDistributed(keras.layers.Dense(num_chars,\n",
    "                                                          activation='softmax')))\n",
    "modelmm.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "modelmm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training time\n",
    "log = modelmm.fit(X, y, epochs=10, batch_size=128)\n",
    "plt.plot(log.history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:  # make sure the seed has the right length\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:            # sample randomly the seed from corpus\n",
    "            seed = random.randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "        return pattern\n",
    "\n",
    "seed_pattern = make_seed(\"In the early morning, the flower is shining\")\n",
    "\n",
    "X = np.zeros((1, sentence_length, num_chars), dtype=float)\n",
    "for i, character in enumerate(seed_pattern):\n",
    "    X[0, i, encoding[character]] = 1\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(500):\n",
    "    output_prob = modelmm.predict(X, verbose=0)[0][-1]\n",
    "    # in previous line predict() gives a tensor of shape (1, 20, 65) \n",
    "    # with 1 being the size of the batch, for that we use [0][-1] \n",
    "    # to remove the batch dim and get the last prediction : a vector of size 65\n",
    "    prediction = np.random.choice(num_chars, p = output_prob )\n",
    "    generated_text += decoding[prediction]\n",
    "    activations = np.zeros((1, 1, num_chars), dtype=bool)\n",
    "    activations[0, 0, prediction] = 1\n",
    "    #now remove first char and glue the predicted one\n",
    "    X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
