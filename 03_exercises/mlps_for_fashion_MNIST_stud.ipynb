{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:03<00:00, 7.31MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 834kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 4.35MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 20.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.mnist.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.mnist.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2da4ed-af9a-4fc7-b59f-135b51241d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b30c81-d89f-4947-aa09-ab8c70dd262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data), len(validation_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832efcb-c191-42f2-93f5-60ff2e99e934",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fixed MLP with Increasing Training Dataset\n",
    "\n",
    "Create a MLP with one hidden layer with 200 units for Fashion MNIST classification. Use ReLU activation.\n",
    "\n",
    "Use a random fraction of the training set (split above) to perform the training. Always use the same validation set.\n",
    "\n",
    "Use SGD and cross-entropy loss and suitable learning rate.\n",
    "\n",
    "Start with a single small batch for training (batch size 8) and make sure that you can overfit, i.e. bring the training accuracy to 100%.\n",
    "\n",
    "Then, gradually increase the training set. Let it grow until you obtain values for the training and the validation loss which no longer indicate overfitting. Use a fixed batch size (batchsize 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65fabc-6c12-4e88-8a47-0f5ce337103d",
   "metadata": {},
   "source": [
    "#### MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cf040-3853-4e48-afa7-4666f8c46d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method that provides an instance of an MLP which uses as a list of units per layer as input\n",
    "\n",
    "def mlp(units = [28*28, 200, 10]):\n",
    "    \"\"\"\n",
    "    Creates an instance of an MLP with layers as specified in the 'units'-list (list of integers).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c108c38-1e25-4353-b95a-1ecb00c43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance and its summary\n",
    "\n",
    "model = mlp()\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfd65c-d84d-401e-a209-8d4de656fee3",
   "metadata": {},
   "source": [
    "#### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6979eb0-1b61-44df-8f83-3ad42bc6e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_data, validation_data):\n",
    "    \"\"\"\n",
    "    Performs the training of a model with given learning rate (lr), \n",
    "    number of epochs (nepochs), batchsize (nbatch) and training and validation data.\n",
    "    Suitable data loaders are instantiated for the training and validation datasets.\n",
    "    Keep book about cost and accuracy (per epoch) for both training and validation set.\n",
    "    \"\"\"\n",
    "    cost_hist = []\n",
    "    cost_hist_test = []\n",
    "    acc_hist = []\n",
    "    acc_hist_test = []\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return cost_hist, cost_hist_test, acc_hist, acc_hist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7a9da-8c34-42c4-8907-3cc7990227e4",
   "metadata": {},
   "source": [
    "#### First Training\n",
    "\n",
    "Run a first training with only one small training batch (e.g. with a single batch of 64 samples). \n",
    "The small training set can be created by using the functionality `torch.utils.data.random_split` already used above. As validation set use the `validation_data` created above.  \n",
    "\n",
    "This training run can be used to test whether the model and training loop are properly implemented. Explain why and in what sense it can be used as test.\n",
    "\n",
    "This is something you can always do when training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37778e0b-7f07-4af8-8f9e-45e6ba864248",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 32\n",
    "nbatches = 1\n",
    "nepochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "trainsize = nbatches*nbatch\n",
    "trainset, rest = torch.utils.data.random_split(training_data, [trainsize, 50000-trainsize])\n",
    "print(len(trainset), len(rest))\n",
    "\n",
    "model = mlp([28*28, 200, 10])\n",
    "cost_train, cost_valid, acc_train, acc_valid = train_eval(model, lr, nepochs, nbatch, trainset, validation_data)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs), cost_train, \"b-\")\n",
    "plt.plot(range(nepochs), cost_valid, \"r-\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs), acc_train, \"b-\")\n",
    "plt.plot(range(nepochs), acc_valid, \"r-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c8ceb-8083-4e62-b123-b026aef32e22",
   "metadata": {},
   "source": [
    "#### Evaluate Train and Validation Performance \n",
    "\n",
    "Now run several trainings with the same small model (one hidden layer) and explore for different number of training samples (different number of batches with 32 samples) used, how the train and validation performance evolve (cost and accuracy). Make sure that you train sufficiently long to obtain representative values for cost and accuracy with the given settings. Always use the same validation set (with 10'000 samples).\n",
    "\n",
    "Create plots with training and validation performance vs number of training batches (one for cost and one for accuracy). Use the performance characteristics obtained at the end.\n",
    "\n",
    "Discuss the whether there is a sufficient number of training samples for the given problem at hand. Specify a minimum number if applicable. Also consider whether you are in the underfitting regime.\n",
    "\n",
    "Hint: Specify a list with the number of training batches you would like to perform trainings. Try to be economic with the resources used - try to keep the number of trainings limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0bb95-7392-46a5-9481-640e9177d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e0c6ba-8f5e-4470-b110-1e52d86baa59",
   "metadata": {},
   "source": [
    "#### Comments: YOUR comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d08b0-8ad2-4fc1-98e8-39dd29f204a1",
   "metadata": {},
   "source": [
    "### Evaluate Different Model Complexities\n",
    "\n",
    "Use the same functionality implemented above (create MLP model, train and evaluate model) to evaluate different model complexities: Number of layers and number of units per layer.\n",
    "\n",
    "Start with the small model used in Exercise 2. Then gradually increase the model complexity. Do this along two dimensions:\n",
    "* a single hidden layer, but increasing the number of units.\n",
    "* a fixed number of units per (hidden) layer, but increase the number of layers.\n",
    "Make sure that you reach the overfitting regime (in either case).\n",
    "\n",
    "Always use the full training set with 50'000 samples.\n",
    "\n",
    "Again make sure that you train sufficiently long so that the obtained train and validation performance measures (cost, accuracy) are representative.\n",
    "\n",
    "Create plots with training and validation performance (cost, accuracy) vs model complexity - one plot with number of units for the single hidden layer case, and one for varying number of layers. \n",
    "\n",
    "Again use the performance characteristics obtained at the end. \n",
    "\n",
    "Finally, discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcd0fc-3fd6-4b07-a594-e7f6a6336e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "# several iterations with the code snippets of the form:\n",
    "\n",
    "nbatch = 64\n",
    "nepochs = \n",
    "lr = \n",
    "\n",
    "model = \n",
    "\n",
    "cost_train, cost_valid, acc_train, acc_valid = train_eval(model, lr, nepochs, nbatch, training_data, validation_data)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs), cost_train, \"b-\")\n",
    "plt.plot(range(nepochs), cost_valid, \"r-\")\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs), acc_train, \"b-\")\n",
    "plt.plot(range(nepochs), acc_valid, \"r-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e7893-e293-4ae6-a983-c335ff0c4b21",
   "metadata": {},
   "source": [
    "#### Comments: YOUR findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61beca-5f13-404c-9c9c-568ccca8a00a",
   "metadata": {},
   "source": [
    "### Add Regularisation\n",
    "\n",
    "Finally, add regularisation - dropout or L1/L2-regularisation. \n",
    "\n",
    "To this end, you need to implement new functionality to instantiate the model.\n",
    "\n",
    "Start with one of the overfitting cases of Exercise 3 and try to improve the validation performance by adding regularisation. You can use either dropout or L1/L2-regularisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b15df-a84b-483d-9365-d8f66390b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method that provides an instance of an MLP incl regularisation which uses as a list of units per layer as input \n",
    "\n",
    "def mlp_dropout(units = [28*28, 200, 10], p_in = 0.2, p_hidden=0.5):\n",
    "    \"\"\"\n",
    "    Creates an instance of an MLP with layers as specified in the 'units'-list (list of integers) and dropout \n",
    "    regularisation. Dropout rate for all layers the same except for the first (p_in). For the output layer \n",
    "    no dropout applied. \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE #\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = mlp_dropout([28*28,200,10])\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa393a-9f99-48f4-996d-cf8d2a3b8cd5",
   "metadata": {},
   "source": [
    "#### Playing with different complexities and regularisation\n",
    "\n",
    "Now play with different complexities and regularisation. \n",
    "Start with one of the overfitting cases identified in the previous exercise.\n",
    "By adding regularisation, you should be able to make it non-overfitting, i.e. generalising better.\n",
    "Note that for a given complexity, adding regularisation reduces the model capacity. This may need to be compensated by increasing the complexity of the model. \n",
    "\n",
    "Use again cost and accuracy for train and validation set to evaluate the results.\n",
    "\n",
    "Finally, estimate the bias error and the generalisation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa1c2b-50d7-4b23-91c8-c47eec5fc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0bef3-7aeb-4329-aa64-e8d107c9bee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
